{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "matdat = loadmat(\"mnistdata.mat\")\n",
    "batch_data = matdat[\"batchdata\"]\n",
    "batch_targets = matdat[\"batchtargets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, d_in, num_batches = batch_data.shape\n",
    "_, num_labels, _= batch_targets.shape\n",
    "\n",
    "class Args():\n",
    "    d_in = d_in\n",
    "    batch_size = batch_size\n",
    "    num_batches = num_batches\n",
    "    trainset_size = batch_size*num_batches\n",
    "    num_labels = num_labels\n",
    "    label_strength = 1 # Hinton's \"labelstrength\", the strength of the layer pixel\n",
    "    min_layer_softmax = 2 # Does not use hidden layers lower than this\n",
    "    min_level_energy = 2 # Used for computing goodness at test time\n",
    "    wc_hidden = 0.001   # Hinton's \"wc\" for forward weights\n",
    "    wc_softmax = 0.003 # Hinton's \"sup_wc\" for label prediction\n",
    "    lr_hidden = 0.01 # Hinton's \"epsilon\"\n",
    "    lr_softmax = 0.1 # Hinton's \"epsilonup\"\n",
    "    grad_smoothing = 0.9 # Hinton's \"delay\" variable, default=0.9\n",
    "    lambda_mean = 0.03 # Peer normalization (WTF is this?)\n",
    "    num_epochs = 100    # Maximum number of epochs; Hinton uses 100\n",
    "    layer_dims = [d_in, 1000, 1000, 1000, num_labels] # Layer sizes for the model (always start with d_in and end with num_lables)\n",
    "    num_layers = len(layer_dims)\n",
    "    temp = 1 # Used for rescaling weights (Not used)\n",
    "    tiny = np.exp(-50)\n",
    "    dtype=np.float32 # Was trying to speed this up, doesn't help much\n",
    "args = Args()\n",
    "\n",
    "def norm_rows(x):\n",
    "    # Makes the sum of squared activations be 1 per neuron\n",
    "    eps = np.exp(-100,  dtype=args.dtype)\n",
    "    num_comp = x.shape[1]\n",
    "    # Considering using vectors of ones instead\n",
    "    # Need to use keepdims for broadcasting\n",
    "    return x / np.tile(eps + np.mean(x ** 2, axis=1, keepdims=True)**(0.5), (1, num_comp))\n",
    "\n",
    "def logistic_fn(x):\n",
    "    return np.divide(1, 1 + np.exp(-1*x))\n",
    "\n",
    "\n",
    "def choose_from_probs(probs):\n",
    "    batch_size, num_labs = probs.shape\n",
    "    rands = np.random.rand(batch_size)\n",
    "    choices = np.zeros(probs.shape, dtype=args.dtype)\n",
    "    for n in range(batch_size):\n",
    "        sum_so_far = 0\n",
    "        used = 0\n",
    "        for lab_idx in range(num_labs):\n",
    "            sum_so_far += probs[n, lab_idx]\n",
    "            if rands[n] < sum_so_far and used == 0:\n",
    "                used = 1\n",
    "                choices[n, lab_idx] = 1\n",
    "                break\n",
    "    return choices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done setting everything up\n"
     ]
    }
   ],
   "source": [
    "net_inputs = {}\n",
    "hidden_states = {}\n",
    "normed_states = {}\n",
    "pos_probs = {}\n",
    "neg_probs = {}\n",
    "weights = {}\n",
    "weights_grad = {}\n",
    "biases = {}\n",
    "biases_grad = {}\n",
    "dC_by_din = {}\n",
    "pos_ex_dC_by_dweights = {}\n",
    "neg_ex_dC_by_dweights = {}\n",
    "pos_ex_dC_by_dbiases = {}\n",
    "neg_ex_dC_by_dbiases = {}\n",
    "mean_states = {}  # Running average of hidden layers\n",
    "softmax_weights = {}\n",
    "softmax_weights_grad = {}\n",
    "\n",
    "# Initialization Loop:\n",
    "for layer_num in range(1, args.num_layers):\n",
    "    d_in = args.layer_dims[layer_num - 1]\n",
    "    d_out = args.layer_dims[layer_num]\n",
    "    net_inputs[layer_num] = np.zeros([args.batch_size, d_out], dtype=args.dtype)\n",
    "    # Keeping track of all the input stuff at each layer\n",
    "    hidden_states[layer_num] = np.zeros([args.batch_size, d_out], dtype=args.dtype)\n",
    "    normed_states[layer_num] = np.zeros([args.batch_size, d_out], dtype=args.dtype)\n",
    "    pos_probs[layer_num] = np.zeros(args.batch_size, dtype=args.dtype)\n",
    "    neg_probs[layer_num] = np.zeros(args.batch_size, dtype=args.dtype)\n",
    "    # Keep track of the network information:\n",
    "    weights[layer_num] = (1/np.sqrt(d_in,  dtype=args.dtype) * np.random.randn(d_in, d_out))\n",
    "    weights_grad[layer_num] = np.zeros([d_in, d_out], dtype=args.dtype)\n",
    "    biases[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    biases_grad[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    pos_ex_dC_by_dweights[layer_num] = np.zeros([d_in, d_out], dtype=args.dtype)\n",
    "    neg_ex_dC_by_dweights[layer_num] = np.zeros([d_in, d_out], dtype=args.dtype)\n",
    "    pos_ex_dC_by_dbiases[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    neg_ex_dC_by_dbiases[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "\n",
    "for layer_num in range(1, args.num_layers):\n",
    "    mean_states[layer_num] = 0.5*np.ones([1, args.layer_dims[layer_num]], dtype=args.dtype)\n",
    "    softmax_weights[layer_num] = np.zeros([args.layer_dims[layer_num], args.num_labels], dtype=args.dtype)\n",
    "    softmax_weights_grad[layer_num] = np.zeros([args.layer_dims[layer_num], args.num_labels], dtype=args.dtype)\n",
    "\n",
    "batch_data.astype(args.dtype, copy=False)\n",
    "batch_targets.astype(args.dtype, copy=False)\n",
    "print(\"Done setting everything up\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nz/r991tz5d1n1fdby4l9mhm76w0000gn/T/ipykernel_8147/1681683655.py:36: RuntimeWarning: overflow encountered in exp\n",
      "  return np.divide(1, 1 + np.exp(-1*x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Errors Epoch 0 = 9242\n",
      "Accuracy = 81.516%\n",
      "Train Errors Epoch 1 = 5433\n",
      "Accuracy = 89.134%\n",
      "Train Errors Epoch 2 = 4829\n",
      "Accuracy = 90.342%\n",
      "Train Errors Epoch 3 = 4564\n",
      "Accuracy = 90.872%\n",
      "Train Errors Epoch 4 = 3254\n",
      "Accuracy = 93.492%\n",
      "Train Errors Epoch 5 = 3108\n",
      "Accuracy = 93.784%\n",
      "Train Errors Epoch 6 = 2937\n",
      "Accuracy = 94.126%\n",
      "Train Errors Epoch 7 = 2622\n",
      "Accuracy = 94.756%\n",
      "Train Errors Epoch 8 = 2390\n",
      "Accuracy = 95.22%\n",
      "Train Errors Epoch 9 = 1970\n",
      "Accuracy = 96.06%\n",
      "Train Errors Epoch 10 = 1670\n",
      "Accuracy = 96.66%\n",
      "Train Errors Epoch 11 = 1791\n",
      "Accuracy = 96.418%\n",
      "Train Errors Epoch 12 = 1579\n",
      "Accuracy = 96.842%\n",
      "Train Errors Epoch 13 = 1392\n",
      "Accuracy = 97.216%\n",
      "Train Errors Epoch 14 = 1448\n",
      "Accuracy = 97.104%\n",
      "Train Errors Epoch 15 = 1294\n",
      "Accuracy = 97.412%\n",
      "Train Errors Epoch 16 = 1310\n",
      "Accuracy = 97.38%\n",
      "Train Errors Epoch 17 = 1132\n",
      "Accuracy = 97.736%\n",
      "Train Errors Epoch 18 = 1279\n",
      "Accuracy = 97.442%\n",
      "Train Errors Epoch 19 = 1121\n",
      "Accuracy = 97.758%\n",
      "Train Errors Epoch 20 = 1084\n",
      "Accuracy = 97.832%\n",
      "Train Errors Epoch 21 = 1062\n",
      "Accuracy = 97.876%\n",
      "Train Errors Epoch 22 = 1000\n",
      "Accuracy = 98.0%\n",
      "Train Errors Epoch 23 = 863\n",
      "Accuracy = 98.274%\n",
      "Train Errors Epoch 24 = 882\n",
      "Accuracy = 98.236%\n",
      "Train Errors Epoch 25 = 763\n",
      "Accuracy = 98.474%\n",
      "Train Errors Epoch 26 = 808\n",
      "Accuracy = 98.384%\n",
      "Train Errors Epoch 27 = 855\n",
      "Accuracy = 98.29%\n",
      "Train Errors Epoch 28 = 815\n",
      "Accuracy = 98.37%\n",
      "Train Errors Epoch 29 = 754\n",
      "Accuracy = 98.492%\n",
      "Train Errors Epoch 30 = 777\n",
      "Accuracy = 98.446%\n",
      "Train Errors Epoch 31 = 677\n",
      "Accuracy = 98.646%\n",
      "Train Errors Epoch 32 = 680\n",
      "Accuracy = 98.64%\n",
      "Train Errors Epoch 33 = 660\n",
      "Accuracy = 98.68%\n",
      "Train Errors Epoch 34 = 642\n",
      "Accuracy = 98.716%\n",
      "Train Errors Epoch 35 = 607\n",
      "Accuracy = 98.786%\n",
      "Train Errors Epoch 36 = 565\n",
      "Accuracy = 98.87%\n",
      "Train Errors Epoch 37 = 517\n",
      "Accuracy = 98.966%\n",
      "Train Errors Epoch 38 = 521\n",
      "Accuracy = 98.958%\n",
      "Train Errors Epoch 39 = 588\n",
      "Accuracy = 98.824%\n",
      "Train Errors Epoch 40 = 541\n",
      "Accuracy = 98.918%\n",
      "Train Errors Epoch 41 = 507\n",
      "Accuracy = 98.986%\n",
      "Train Errors Epoch 42 = 478\n",
      "Accuracy = 99.044%\n",
      "Train Errors Epoch 43 = 472\n",
      "Accuracy = 99.056%\n",
      "Train Errors Epoch 44 = 447\n",
      "Accuracy = 99.106%\n",
      "Train Errors Epoch 45 = 400\n",
      "Accuracy = 99.2%\n",
      "Train Errors Epoch 46 = 385\n",
      "Accuracy = 99.23%\n",
      "Train Errors Epoch 47 = 436\n",
      "Accuracy = 99.128%\n",
      "Train Errors Epoch 48 = 388\n",
      "Accuracy = 99.224%\n",
      "Train Errors Epoch 49 = 378\n",
      "Accuracy = 99.244%\n",
      "Train Errors Epoch 50 = 382\n",
      "Accuracy = 99.236%\n",
      "Train Errors Epoch 51 = 362\n",
      "Accuracy = 99.276%\n",
      "Train Errors Epoch 52 = 381\n",
      "Accuracy = 99.238%\n",
      "Train Errors Epoch 53 = 306\n",
      "Accuracy = 99.388%\n",
      "Train Errors Epoch 54 = 314\n",
      "Accuracy = 99.372%\n",
      "Train Errors Epoch 55 = 262\n",
      "Accuracy = 99.476%\n",
      "Train Errors Epoch 56 = 245\n",
      "Accuracy = 99.51%\n",
      "Train Errors Epoch 57 = 250\n",
      "Accuracy = 99.5%\n",
      "Train Errors Epoch 58 = 256\n",
      "Accuracy = 99.488%\n",
      "Train Errors Epoch 59 = 165\n",
      "Accuracy = 99.67%\n",
      "Train Errors Epoch 60 = 158\n",
      "Accuracy = 99.684%\n",
      "Train Errors Epoch 61 = 170\n",
      "Accuracy = 99.66%\n",
      "Train Errors Epoch 62 = 150\n",
      "Accuracy = 99.7%\n",
      "Train Errors Epoch 63 = 124\n",
      "Accuracy = 99.752%\n",
      "Train Errors Epoch 64 = 130\n",
      "Accuracy = 99.74%\n",
      "Train Errors Epoch 65 = 94\n",
      "Accuracy = 99.812%\n",
      "Train Errors Epoch 66 = 87\n",
      "Accuracy = 99.826%\n",
      "Train Errors Epoch 67 = 66\n",
      "Accuracy = 99.868%\n",
      "Train Errors Epoch 68 = 68\n",
      "Accuracy = 99.864%\n",
      "Train Errors Epoch 69 = 71\n",
      "Accuracy = 99.858%\n",
      "Train Errors Epoch 70 = 63\n",
      "Accuracy = 99.874%\n",
      "Train Errors Epoch 71 = 64\n",
      "Accuracy = 99.872%\n",
      "Train Errors Epoch 72 = 42\n",
      "Accuracy = 99.916%\n",
      "Train Errors Epoch 73 = 44\n",
      "Accuracy = 99.912%\n",
      "Train Errors Epoch 74 = 45\n",
      "Accuracy = 99.91%\n",
      "Train Errors Epoch 75 = 29\n",
      "Accuracy = 99.942%\n",
      "Train Errors Epoch 76 = 38\n",
      "Accuracy = 99.924%\n",
      "Train Errors Epoch 77 = 29\n",
      "Accuracy = 99.942%\n",
      "Train Errors Epoch 78 = 25\n",
      "Accuracy = 99.95%\n",
      "Train Errors Epoch 79 = 28\n",
      "Accuracy = 99.944%\n",
      "Train Errors Epoch 80 = 16\n",
      "Accuracy = 99.968%\n",
      "Train Errors Epoch 81 = 20\n",
      "Accuracy = 99.96%\n",
      "Train Errors Epoch 82 = 17\n",
      "Accuracy = 99.966%\n",
      "Train Errors Epoch 83 = 16\n",
      "Accuracy = 99.968%\n",
      "Train Errors Epoch 84 = 7\n",
      "Accuracy = 99.986%\n",
      "Train Errors Epoch 85 = 7\n",
      "Accuracy = 99.986%\n",
      "Train Errors Epoch 86 = 12\n",
      "Accuracy = 99.976%\n",
      "Train Errors Epoch 87 = 7\n",
      "Accuracy = 99.986%\n",
      "Train Errors Epoch 88 = 9\n",
      "Accuracy = 99.982%\n",
      "Train Errors Epoch 89 = 5\n",
      "Accuracy = 99.99%\n",
      "Train Errors Epoch 90 = 7\n",
      "Accuracy = 99.986%\n",
      "Train Errors Epoch 91 = 6\n",
      "Accuracy = 99.988%\n",
      "Train Errors Epoch 92 = 5\n",
      "Accuracy = 99.99%\n",
      "Train Errors Epoch 93 = 6\n",
      "Accuracy = 99.988%\n",
      "Train Errors Epoch 94 = 4\n",
      "Accuracy = 99.992%\n",
      "Train Errors Epoch 95 = 4\n",
      "Accuracy = 99.992%\n",
      "Train Errors Epoch 96 = 5\n",
      "Accuracy = 99.99%\n",
      "Train Errors Epoch 97 = 5\n",
      "Accuracy = 99.99%\n",
      "Train Errors Epoch 98 = 5\n",
      "Accuracy = 99.99%\n",
      "Train Errors Epoch 99 = 4\n",
      "Accuracy = 99.992%\n",
      "Donesies Train Errors = 4\n",
      "Accuracy = 99.992%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.num_epochs):\n",
    "    pos_errors = np.zeros(args.num_layers, dtype=args.dtype)\n",
    "    neg_errors = np.zeros(args.num_layers, dtype=args.dtype)\n",
    "    pair_sum_errors = np.zeros(args.num_layers, dtype=args.dtype)\n",
    "    train_log_cost = 0\n",
    "    train_errors = 0\n",
    "    #  Linear decaying lr after first half of training; Hinton's \"epsgain\" parameter\n",
    "    if epoch < args.num_epochs/2.0:\n",
    "        weight_mult = 1 # This is Hinton's \"epsgain\" parameter\n",
    "    else:\n",
    "        weight_mult = (1 + 2.0*(args.num_epochs - epoch))/args.num_epochs\n",
    "    for batch in range(args.num_batches):\n",
    "        x = batch_data[:, :, batch]\n",
    "        y = batch_targets[:, :, batch]\n",
    "        # Now add the target label to the image (in the first few pixels)\n",
    "        x[:, 0:args.num_labels] = args.label_strength * y\n",
    "        normed_states[0] = norm_rows(x)\n",
    "        for layer_num in range(1, args.num_layers):\n",
    "            # Forward Pass for each layer\n",
    "            net_inputs[layer_num] = normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num]\n",
    "            # ReLU\n",
    "            relu_output = np.maximum(net_inputs[layer_num], 0)\n",
    "            hidden_states[layer_num] = relu_output\n",
    "            normed_states[layer_num] = norm_rows(hidden_states[layer_num])\n",
    "            goodness = np.sum(hidden_states[layer_num]**2, axis=1, keepdims=True)\n",
    "            # Really not sure why we are subtracting this\n",
    "            pos_probs[layer_num] = (logistic_fn(goodness - (args.layer_dims[layer_num]/args.temp)))\n",
    "            dC_by_din[layer_num] = np.tile(1 - pos_probs[layer_num], (1, args.layer_dims[layer_num]))*relu_output\n",
    "            mean_states[layer_num] = 0.9*mean_states[layer_num] + 0.1*np.mean(relu_output, axis=0, keepdims=True)\n",
    "            # Regularizer encouraging layers to turn on\n",
    "            dC_by_din[layer_num] += args.lambda_mean*(np.mean(mean_states[layer_num]) - mean_states[layer_num])\n",
    "            pos_ex_dC_by_dweights[layer_num] = normed_states[layer_num - 1].T @ dC_by_din[layer_num]\n",
    "            pos_ex_dC_by_dbiases[layer_num] = np.sum(dC_by_din[layer_num], axis=0, keepdims=True)\n",
    "        # Now get hidden states when label is neutral.  Use this to pick hard negative labels\n",
    "        x[:, 0:args.num_labels] = args.label_strength * np.ones([args.batch_size, args.num_labels], dtype=args.dtype)/args.num_labels\n",
    "        normed_states[0] = norm_rows(x)\n",
    "        # Run the neutral data through the forward pass\n",
    "        for layer_num in range(1, args.num_layers):\n",
    "            net_inputs[layer_num] = normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num]\n",
    "            relu_output = np.maximum(net_inputs[layer_num], 0)\n",
    "            hidden_states[layer_num] = relu_output\n",
    "            normed_states[layer_num] = norm_rows(relu_output)\n",
    "        # Looks like we use the last layer to calculate the negative data, some sort of feedback\n",
    "        label_in = np.tile(biases[args.num_layers - 1], (args.batch_size, 1))\n",
    "        for layer_num in range(args.min_layer_softmax, args.num_layers):\n",
    "            label_in += normed_states[layer_num] @ softmax_weights[layer_num]\n",
    "        label_in = label_in - np.tile(np.max(label_in, axis=1, keepdims=True), (1, args.num_labels))\n",
    "        unnorm_probs = np.exp(label_in)\n",
    "        train_predictions = unnorm_probs/np.tile(np.sum(unnorm_probs, axis=1, keepdims=True), (1, args.num_labels))\n",
    "        correct_probs = np.sum(train_predictions*y, axis=1, keepdims=True) # Should be a Column vector\n",
    "        curr_train_log_cost = -1 * np.log(args.tiny + correct_probs)\n",
    "        train_log_cost += np.sum(curr_train_log_cost)/args.num_batches\n",
    "        train_guesses = np.argmax(train_predictions, axis=1)\n",
    "        target_indices = np.argmax(y, axis=1)\n",
    "        train_errors += np.sum(train_guesses != target_indices)\n",
    "        # Now do the backprop step\n",
    "        dC_by_din[args.num_layers] = y - train_predictions\n",
    "        # Not used:\n",
    "        # dC_by_dbiases = sum(dC_by_din[args.num_layers], axis=0)\n",
    "        for layer_num in range(args.min_layer_softmax, args.num_layers):\n",
    "            dC_by_softmax_weights = normed_states[layer_num].T @ dC_by_din[args.num_layers]\n",
    "            softmax_weights_grad[layer_num] = args.grad_smoothing * softmax_weights_grad[layer_num] + (1 - args.grad_smoothing)*dC_by_softmax_weights/args.batch_size\n",
    "            softmax_weights[layer_num] += weight_mult*args.lr_softmax*(softmax_weights_grad[layer_num] - args.wc_softmax*softmax_weights[layer_num])\n",
    "        # Make Negative Data\n",
    "        neg_data = x\n",
    "        # Big negative logits\n",
    "        label_in_others = label_in - 1000*y\n",
    "        neg_data[:, :args.num_labels] = args.label_strength*choose_from_probs(np.exp(label_in_others)/sum(np.exp(label_in_others)))\n",
    "        normed_states[0] = norm_rows(neg_data)\n",
    "        for layer_num in range(1, args.num_layers):\n",
    "            net_inputs[layer_num] = normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num]\n",
    "            relu_output = np.maximum(net_inputs[layer_num], 0)\n",
    "            hidden_states[layer_num] = relu_output\n",
    "            normed_states[layer_num] = norm_rows(relu_output)\n",
    "            goodness = np.sum(hidden_states[layer_num]**2, axis=1, keepdims=True)\n",
    "            # Really not sure why we are subtracting this\n",
    "            neg_probs[layer_num] = (logistic_fn(goodness - (args.layer_dims[layer_num]/args.temp)))\n",
    "            dC_by_din[layer_num] = np.tile(-1*neg_probs[layer_num], (1, args.layer_dims[layer_num]))*relu_output\n",
    "            neg_ex_dC_by_dweights[layer_num] = normed_states[layer_num - 1].T @ dC_by_din[layer_num]\n",
    "            neg_ex_dC_by_dbiases[layer_num] = np.sum(dC_by_din[layer_num], axis=0, keepdims=True)\n",
    "            pair_sum_errors[layer_num] += np.sum(neg_probs[layer_num] > pos_probs[layer_num])\n",
    "        # Gradient Backprop (FINALLY)\n",
    "        for layer_num in range(1, args.num_layers):\n",
    "            dC_by_dW = (pos_ex_dC_by_dweights[layer_num] + neg_ex_dC_by_dweights[layer_num])/args.batch_size\n",
    "            weights_grad[layer_num] = args.grad_smoothing*weights_grad[layer_num] + (1 - args.grad_smoothing)*(dC_by_dW)\n",
    "            \n",
    "            dC_by_dB = (pos_ex_dC_by_dbiases[layer_num] + neg_ex_dC_by_dbiases[layer_num])/args.batch_size\n",
    "            biases_grad[layer_num] = args.grad_smoothing*biases_grad[layer_num] + (1 - args.grad_smoothing)*dC_by_dB\n",
    "            biases[layer_num] += args.lr_hidden*weight_mult*biases_grad[layer_num]\n",
    "            weights[layer_num] += args.lr_hidden*weight_mult*(weights_grad[layer_num] - args.wc_hidden*weights[layer_num])\n",
    "    print(f\"Train Errors Epoch {epoch} = {train_errors}\")\n",
    "    print(f\"Accuracy = {100.0*(args.trainset_size-train_errors)/(1.0*args.trainset_size)}%\")\n",
    "print(f\"Donesies Train Errors = {train_errors}\")\n",
    "print(f\"Accuracy = {100.0*(args.trainset_size-train_errors)/(1.0*args.trainset_size)}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For viewing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'batchdata', 'batchtargets', 'finaltestbatchdata', 'finaltestbatchtargets', 'validbatchdata', 'validbatchtargets'])\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(x[3, :].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing errors = 168\n",
      "Test error percentage = 0.0168\n",
      "Test Accuracy = 0.9832\n"
     ]
    }
   ],
   "source": [
    "# Energy testing\n",
    "def calc_errors(batch_data, batch_targets, args):\n",
    "    normed_states = {}\n",
    "    hidden_states = {}\n",
    "    _, _, num_batches = batch_data.shape\n",
    "    _, num_labels, _= batch_targets.shape\n",
    "    num_errors = 0\n",
    "    for batch_idx in range(num_batches):\n",
    "        x = batch_data[:, :, batch_idx]\n",
    "        y = batch_targets[:, :, batch_idx]\n",
    "        act_sum_sqrs = np.zeros([num_batches, num_labels])\n",
    "        for label_num in range(num_labels):\n",
    "            x[:, :num_labels] = np.zeros([num_batches, num_labels])\n",
    "            x[:, label_num] = args.label_strength * np.ones([num_batches])\n",
    "            normed_states[0] = norm_rows(x)\n",
    "            for layer_num in range(1, args.num_layers - 1):\n",
    "                pre_relu = normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num]\n",
    "                hidden_states[layer_num] = np.maximum(0, pre_relu)\n",
    "                if layer_num >= args.min_level_energy:\n",
    "                    act_sum_sqrs[:, label_num] += np.sum(hidden_states[layer_num]**2, axis=1)\n",
    "                normed_states[layer_num] = norm_rows(hidden_states[layer_num])\n",
    "        guesses = np.argmax(act_sum_sqrs, axis=1)\n",
    "        targets_argmax = np.argmax(y, axis=1)\n",
    "        num_errors += np.sum(guesses != targets_argmax)\n",
    "    return num_errors\n",
    "\n",
    "test_x = matdat['finaltestbatchdata'].astype(args.dtype, copy=False)\n",
    "test_y = matdat['finaltestbatchtargets'].astype(args.dtype, copy=False)\n",
    "test_batch_size, _, test_num_batches = test_x.shape\n",
    "num_test_data = test_batch_size*test_num_batches\n",
    "\n",
    "test_errors = calc_errors(batch_data=test_x, batch_targets=test_y, args=args)\n",
    "\n",
    "print(f\"Number of testing errors = {test_errors}\")\n",
    "print(f\"Test error percentage = {test_errors*100.0/num_test_data}\")\n",
    "print(f\"Test Accuracy = {100 - (test_errors*100.0/num_test_data)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04ffba0d41b4216980bf6c7d893a68b19ea460a3309261abb88c05c7a783f5c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
