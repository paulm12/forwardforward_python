{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "matdat = loadmat(\"mnistdata.mat\")\n",
    "batch_data = matdat[\"batchdata\"]\n",
    "batch_targets = matdat[\"batchtargets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, d_in, num_batches = batch_data.shape\n",
    "_, num_classes, _= batch_targets.shape\n",
    "\n",
    "class Args():\n",
    "    d_in = d_in\n",
    "    batch_size = batch_size\n",
    "    num_batches = num_batches\n",
    "    trainset_size = batch_size*num_batches\n",
    "    num_classes = num_classes\n",
    "    label_strength = 1 # Hinton's \"labelstrength\", the strength of the layer pixel\n",
    "    min_layer_softmax = 1 # Does not use hidden layers lower than this\n",
    "    min_layer_energy = 1 # Used for computing goodness at test time\n",
    "    wc_hidden = 0.001   # Hinton's \"wc\" for forward weights\n",
    "    wc_softmax = 0.003 # Hinton's \"sup_wc\" for label prediction\n",
    "    lr_hidden = 0.01 # Hinton's \"epsilon\"\n",
    "    lr_softmax = 0.1 # Hinton's \"epsilonup\"\n",
    "    grad_smoothing = 0.9 # Hinton's \"delay\" variable, default=0.9\n",
    "    lambda_mean = 0.03 # Peer normalization (WTF is this?)\n",
    "    num_epochs = 100    # Maximum number of epochs; Hinton uses 100\n",
    "    layer_dims = [d_in, 1000, 1000, 1000, num_classes] # Layer sizes for the model (always start with d_in and end with num_lables)\n",
    "    num_layers = len(layer_dims) - 1\n",
    "    temp = 1 # Used for rescaling weights (Not used)\n",
    "    tiny = np.exp(-50)\n",
    "    dtype=np.float32 # Was trying to speed this up, doesn't help much\n",
    "args = Args()\n",
    "\n",
    "def norm_rows(x):\n",
    "    # Makes the sum of squared activations be 1 per neuron\n",
    "    eps = np.exp(-100,  dtype=args.dtype)\n",
    "    num_comp = x.shape[1]\n",
    "    # Considering using vectors of ones instead\n",
    "    # Need to use keepdims for broadcasting\n",
    "    return x / np.tile(eps + np.mean(x ** 2, axis=1, keepdims=True)**(0.5), (1, num_comp))\n",
    "\n",
    "def logistic_fn(x):\n",
    "    return np.divide(1, 1 + np.exp(-1*x))\n",
    "\n",
    "\n",
    "def choose_from_probs(probs):\n",
    "    batch_size, num_labs = probs.shape\n",
    "    rands = np.random.rand(batch_size)\n",
    "    choices = np.zeros(probs.shape, dtype=args.dtype)\n",
    "    for n in range(batch_size):\n",
    "        sum_so_far = 0\n",
    "        used = 0\n",
    "        for lab_idx in range(num_labs):\n",
    "            sum_so_far += probs[n, lab_idx]\n",
    "            if rands[n] < sum_so_far and used == 0:\n",
    "                used = 1\n",
    "                choices[n, lab_idx] = 1\n",
    "                break\n",
    "    return choices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done setting everything up\n"
     ]
    }
   ],
   "source": [
    "net_inputs = {}\n",
    "hidden_states = {}\n",
    "normed_states = {}\n",
    "pos_probs = {}\n",
    "neg_probs = {}\n",
    "weights = {}\n",
    "weights_grad = {}\n",
    "biases = {}\n",
    "biases_grad = {}\n",
    "dC_by_din = {}\n",
    "pos_ex_dC_by_dweights = {}\n",
    "neg_ex_dC_by_dweights = {}\n",
    "pos_ex_dC_by_dbiases = {}\n",
    "neg_ex_dC_by_dbiases = {}\n",
    "mean_states = {}  # Running average of hidden layers\n",
    "softmax_weights = {}\n",
    "softmax_weights_grad = {}\n",
    "\n",
    "# Initialization Loop:\n",
    "for layer_num in range(args.num_layers):\n",
    "    d_in = args.layer_dims[layer_num]\n",
    "    d_out = args.layer_dims[layer_num + 1]\n",
    "    net_inputs[layer_num] = np.zeros([args.batch_size, d_out], dtype=args.dtype)\n",
    "    # Keeping track of all the input stuff at each layer\n",
    "    hidden_states[layer_num] = np.zeros([args.batch_size, d_out], dtype=args.dtype)\n",
    "    normed_states[layer_num] = np.zeros([args.batch_size, d_out], dtype=args.dtype)\n",
    "    pos_probs[layer_num] = np.zeros(args.batch_size, dtype=args.dtype)\n",
    "    neg_probs[layer_num] = np.zeros(args.batch_size, dtype=args.dtype)\n",
    "    # Keep track of the network information:\n",
    "    weights[layer_num] = (1/np.sqrt(d_in,  dtype=args.dtype) * np.random.randn(d_in, d_out))\n",
    "    weights_grad[layer_num] = np.zeros([d_in, d_out], dtype=args.dtype)\n",
    "    biases[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    biases_grad[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    pos_ex_dC_by_dweights[layer_num] = np.zeros([d_in, d_out], dtype=args.dtype)\n",
    "    neg_ex_dC_by_dweights[layer_num] = np.zeros([d_in, d_out], dtype=args.dtype)\n",
    "    pos_ex_dC_by_dbiases[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    neg_ex_dC_by_dbiases[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    mean_states[layer_num] = 0.5*np.ones([1, d_out], dtype=args.dtype)\n",
    "    softmax_weights[layer_num] = np.zeros([d_out, args.num_classes], dtype=args.dtype)\n",
    "    softmax_weights_grad[layer_num] = np.zeros([d_out, args.num_classes], dtype=args.dtype)\n",
    "\n",
    "batch_data.astype(args.dtype, copy=False)\n",
    "batch_targets.astype(args.dtype, copy=False)\n",
    "print(\"Done setting everything up\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9x/93y24nln7mjb1qs1mnw_0_f00000gn/T/ipykernel_32448/955794127.py:36: RuntimeWarning: overflow encountered in exp\n",
      "  return np.divide(1, 1 + np.exp(-1*x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Errors Epoch 0 = 10788\n",
      "Accuracy = 78.424%\n",
      "Train Errors Epoch 1 = 5131\n",
      "Accuracy = 89.738%\n",
      "Train Errors Epoch 2 = 4228\n",
      "Accuracy = 91.544%\n",
      "Train Errors Epoch 3 = 3545\n",
      "Accuracy = 92.91%\n",
      "Train Errors Epoch 4 = 3080\n",
      "Accuracy = 93.84%\n",
      "Train Errors Epoch 5 = 2842\n",
      "Accuracy = 94.316%\n",
      "Train Errors Epoch 6 = 2641\n",
      "Accuracy = 94.718%\n",
      "Train Errors Epoch 7 = 2279\n",
      "Accuracy = 95.442%\n",
      "Train Errors Epoch 8 = 2222\n",
      "Accuracy = 95.556%\n",
      "Train Errors Epoch 9 = 2049\n",
      "Accuracy = 95.902%\n",
      "Train Errors Epoch 10 = 1978\n",
      "Accuracy = 96.044%\n",
      "Train Errors Epoch 11 = 1660\n",
      "Accuracy = 96.68%\n",
      "Train Errors Epoch 12 = 1633\n",
      "Accuracy = 96.734%\n",
      "Train Errors Epoch 13 = 1494\n",
      "Accuracy = 97.012%\n",
      "Train Errors Epoch 14 = 1649\n",
      "Accuracy = 96.702%\n",
      "Train Errors Epoch 15 = 1396\n",
      "Accuracy = 97.208%\n",
      "Train Errors Epoch 16 = 1575\n",
      "Accuracy = 96.85%\n",
      "Train Errors Epoch 17 = 1481\n",
      "Accuracy = 97.038%\n",
      "Train Errors Epoch 18 = 1444\n",
      "Accuracy = 97.112%\n",
      "Train Errors Epoch 19 = 1236\n",
      "Accuracy = 97.528%\n",
      "Train Errors Epoch 20 = 1250\n",
      "Accuracy = 97.5%\n",
      "Train Errors Epoch 21 = 1155\n",
      "Accuracy = 97.69%\n",
      "Train Errors Epoch 22 = 1184\n",
      "Accuracy = 97.632%\n",
      "Train Errors Epoch 23 = 1051\n",
      "Accuracy = 97.898%\n",
      "Train Errors Epoch 24 = 970\n",
      "Accuracy = 98.06%\n",
      "Train Errors Epoch 25 = 928\n",
      "Accuracy = 98.144%\n",
      "Train Errors Epoch 26 = 854\n",
      "Accuracy = 98.292%\n",
      "Train Errors Epoch 27 = 843\n",
      "Accuracy = 98.314%\n",
      "Train Errors Epoch 28 = 766\n",
      "Accuracy = 98.468%\n",
      "Train Errors Epoch 29 = 744\n",
      "Accuracy = 98.512%\n",
      "Train Errors Epoch 30 = 751\n",
      "Accuracy = 98.498%\n",
      "Train Errors Epoch 31 = 796\n",
      "Accuracy = 98.408%\n",
      "Train Errors Epoch 32 = 774\n",
      "Accuracy = 98.452%\n",
      "Train Errors Epoch 33 = 671\n",
      "Accuracy = 98.658%\n",
      "Train Errors Epoch 34 = 619\n",
      "Accuracy = 98.762%\n",
      "Train Errors Epoch 35 = 607\n",
      "Accuracy = 98.786%\n",
      "Train Errors Epoch 36 = 625\n",
      "Accuracy = 98.75%\n",
      "Train Errors Epoch 37 = 634\n",
      "Accuracy = 98.732%\n",
      "Train Errors Epoch 38 = 560\n",
      "Accuracy = 98.88%\n",
      "Train Errors Epoch 39 = 512\n",
      "Accuracy = 98.976%\n",
      "Train Errors Epoch 40 = 586\n",
      "Accuracy = 98.828%\n",
      "Train Errors Epoch 41 = 639\n",
      "Accuracy = 98.722%\n",
      "Train Errors Epoch 42 = 578\n",
      "Accuracy = 98.844%\n",
      "Train Errors Epoch 43 = 567\n",
      "Accuracy = 98.866%\n",
      "Train Errors Epoch 44 = 468\n",
      "Accuracy = 99.064%\n",
      "Train Errors Epoch 45 = 446\n",
      "Accuracy = 99.108%\n",
      "Train Errors Epoch 46 = 433\n",
      "Accuracy = 99.134%\n",
      "Train Errors Epoch 47 = 454\n",
      "Accuracy = 99.092%\n",
      "Train Errors Epoch 48 = 471\n",
      "Accuracy = 99.058%\n",
      "Train Errors Epoch 49 = 394\n",
      "Accuracy = 99.212%\n",
      "Train Errors Epoch 50 = 368\n",
      "Accuracy = 99.264%\n",
      "Train Errors Epoch 51 = 377\n",
      "Accuracy = 99.246%\n",
      "Train Errors Epoch 52 = 310\n",
      "Accuracy = 99.38%\n",
      "Train Errors Epoch 53 = 342\n",
      "Accuracy = 99.316%\n",
      "Train Errors Epoch 54 = 295\n",
      "Accuracy = 99.41%\n",
      "Train Errors Epoch 55 = 263\n",
      "Accuracy = 99.474%\n",
      "Train Errors Epoch 56 = 246\n",
      "Accuracy = 99.508%\n",
      "Train Errors Epoch 57 = 263\n",
      "Accuracy = 99.474%\n",
      "Train Errors Epoch 58 = 242\n",
      "Accuracy = 99.516%\n",
      "Train Errors Epoch 59 = 193\n",
      "Accuracy = 99.614%\n",
      "Train Errors Epoch 60 = 204\n",
      "Accuracy = 99.592%\n",
      "Train Errors Epoch 61 = 173\n",
      "Accuracy = 99.654%\n",
      "Train Errors Epoch 62 = 170\n",
      "Accuracy = 99.66%\n",
      "Train Errors Epoch 63 = 139\n",
      "Accuracy = 99.722%\n",
      "Train Errors Epoch 64 = 124\n",
      "Accuracy = 99.752%\n",
      "Train Errors Epoch 65 = 133\n",
      "Accuracy = 99.734%\n",
      "Train Errors Epoch 66 = 102\n",
      "Accuracy = 99.796%\n",
      "Train Errors Epoch 67 = 81\n",
      "Accuracy = 99.838%\n",
      "Train Errors Epoch 68 = 79\n",
      "Accuracy = 99.842%\n",
      "Train Errors Epoch 69 = 71\n",
      "Accuracy = 99.858%\n",
      "Train Errors Epoch 70 = 77\n",
      "Accuracy = 99.846%\n",
      "Train Errors Epoch 71 = 79\n",
      "Accuracy = 99.842%\n",
      "Train Errors Epoch 72 = 56\n",
      "Accuracy = 99.888%\n",
      "Train Errors Epoch 73 = 45\n",
      "Accuracy = 99.91%\n",
      "Train Errors Epoch 74 = 60\n",
      "Accuracy = 99.88%\n",
      "Train Errors Epoch 75 = 45\n",
      "Accuracy = 99.91%\n",
      "Train Errors Epoch 76 = 35\n",
      "Accuracy = 99.93%\n",
      "Train Errors Epoch 77 = 41\n",
      "Accuracy = 99.918%\n",
      "Train Errors Epoch 78 = 36\n",
      "Accuracy = 99.928%\n",
      "Train Errors Epoch 79 = 28\n",
      "Accuracy = 99.944%\n",
      "Train Errors Epoch 80 = 25\n",
      "Accuracy = 99.95%\n",
      "Train Errors Epoch 81 = 23\n",
      "Accuracy = 99.954%\n",
      "Train Errors Epoch 82 = 15\n",
      "Accuracy = 99.97%\n",
      "Train Errors Epoch 83 = 9\n",
      "Accuracy = 99.982%\n",
      "Train Errors Epoch 84 = 10\n",
      "Accuracy = 99.98%\n",
      "Train Errors Epoch 85 = 14\n",
      "Accuracy = 99.972%\n",
      "Train Errors Epoch 86 = 12\n",
      "Accuracy = 99.976%\n",
      "Train Errors Epoch 87 = 16\n",
      "Accuracy = 99.968%\n",
      "Train Errors Epoch 88 = 9\n",
      "Accuracy = 99.982%\n",
      "Train Errors Epoch 89 = 7\n",
      "Accuracy = 99.986%\n",
      "Train Errors Epoch 90 = 6\n",
      "Accuracy = 99.988%\n",
      "Train Errors Epoch 91 = 5\n",
      "Accuracy = 99.99%\n",
      "Train Errors Epoch 92 = 5\n",
      "Accuracy = 99.99%\n",
      "Train Errors Epoch 93 = 4\n",
      "Accuracy = 99.992%\n",
      "Train Errors Epoch 94 = 4\n",
      "Accuracy = 99.992%\n",
      "Train Errors Epoch 95 = 5\n",
      "Accuracy = 99.99%\n",
      "Train Errors Epoch 96 = 2\n",
      "Accuracy = 99.996%\n",
      "Train Errors Epoch 97 = 3\n",
      "Accuracy = 99.994%\n",
      "Train Errors Epoch 98 = 3\n",
      "Accuracy = 99.994%\n",
      "Train Errors Epoch 99 = 2\n",
      "Accuracy = 99.996%\n",
      "Donesies Train Errors = 2\n",
      "Accuracy = 99.996%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.num_epochs):\n",
    "    pos_errors = np.zeros(args.num_layers, dtype=args.dtype)\n",
    "    neg_errors = np.zeros(args.num_layers, dtype=args.dtype)\n",
    "    pair_sum_errors = np.zeros(args.num_layers, dtype=args.dtype)\n",
    "    train_log_cost = 0\n",
    "    train_errors = 0\n",
    "    #  Linear decaying lr after first half of training; Hinton's \"epsgain\" parameter\n",
    "    if epoch < args.num_epochs/2.0:\n",
    "        weight_mult = 1 # This is Hinton's \"epsgain\" parameter\n",
    "    else:\n",
    "        weight_mult = (1 + 2.0*(args.num_epochs - epoch))/args.num_epochs\n",
    "    for batch in range(args.num_batches):\n",
    "        x = batch_data[:, :, batch]\n",
    "        y = batch_targets[:, :, batch]\n",
    "        # Now add the target label to the image (in the first few pixels)\n",
    "        x[:, 0:args.num_classes] = args.label_strength * y\n",
    "        normed_states[-1] = norm_rows(x)\n",
    "        for layer_num in range(args.num_layers):\n",
    "            # Forward Pass for each layer\n",
    "            net_inputs[layer_num] = normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num]\n",
    "            # ReLU\n",
    "            relu_output = np.maximum(net_inputs[layer_num], 0)\n",
    "            hidden_states[layer_num] = relu_output\n",
    "            normed_states[layer_num] = norm_rows(hidden_states[layer_num])\n",
    "            goodness = np.sum(hidden_states[layer_num]**2, axis=1, keepdims=True)\n",
    "            # Really not sure why we are subtracting this\n",
    "            pos_probs[layer_num] = (logistic_fn(goodness - (args.layer_dims[layer_num + 1]/args.temp)))\n",
    "            dC_by_din[layer_num] = np.tile(1 - pos_probs[layer_num], (1, args.layer_dims[layer_num + 1]))*relu_output\n",
    "            mean_states[layer_num] = 0.9*mean_states[layer_num] + 0.1*np.mean(relu_output, axis=0, keepdims=True)\n",
    "            # Regularizer encouraging layers to turn on\n",
    "            dC_by_din[layer_num] += args.lambda_mean*(np.mean(mean_states[layer_num]) - mean_states[layer_num])\n",
    "            pos_ex_dC_by_dweights[layer_num] = normed_states[layer_num - 1].T @ dC_by_din[layer_num]\n",
    "            pos_ex_dC_by_dbiases[layer_num] = np.sum(dC_by_din[layer_num], axis=0, keepdims=True)\n",
    "        # Now get hidden states when label is neutral.  Use this to pick hard negative labels\n",
    "        x[:, 0:args.num_classes] = args.label_strength * np.ones([args.batch_size, args.num_classes], dtype=args.dtype)/args.num_classes\n",
    "        normed_states[-1] = norm_rows(x)\n",
    "        # Run the neutral data through the forward pass\n",
    "        for layer_num in range(args.num_layers):\n",
    "            net_inputs[layer_num] = normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num]\n",
    "            relu_output = np.maximum(net_inputs[layer_num], 0)\n",
    "            hidden_states[layer_num] = relu_output\n",
    "            normed_states[layer_num] = norm_rows(relu_output)\n",
    "        # Looks like we use the last layer to calculate the negative data, some sort of feedback\n",
    "        label_in = np.tile(biases[args.num_layers - 1], (args.batch_size, 1))\n",
    "        for layer_num in range(args.min_layer_softmax, args.num_layers):\n",
    "            label_in += normed_states[layer_num] @ softmax_weights[layer_num]\n",
    "        label_in = label_in - np.tile(np.max(label_in, axis=1, keepdims=True), (1, args.num_classes))\n",
    "        unnorm_probs = np.exp(label_in)\n",
    "        train_predictions = unnorm_probs/np.tile(np.sum(unnorm_probs, axis=1, keepdims=True), (1, args.num_classes))\n",
    "        correct_probs = np.sum(train_predictions*y, axis=1, keepdims=True) # Should be a Column vector\n",
    "        curr_train_log_cost = -1 * np.log(args.tiny + correct_probs)\n",
    "        train_log_cost += np.sum(curr_train_log_cost)/args.num_batches\n",
    "        train_guesses = np.argmax(train_predictions, axis=1)\n",
    "        target_indices = np.argmax(y, axis=1)\n",
    "        train_errors += np.sum(train_guesses != target_indices)\n",
    "        # Now do the backprop step\n",
    "        dC_by_din[args.num_layers] = y - train_predictions\n",
    "        # Not used:\n",
    "        # dC_by_dbiases = sum(dC_by_din[args.num_layers], axis=0)\n",
    "        for layer_num in range(args.min_layer_softmax, args.num_layers):\n",
    "            dC_by_softmax_weights = normed_states[layer_num].T @ dC_by_din[args.num_layers]\n",
    "            softmax_weights_grad[layer_num] = args.grad_smoothing * softmax_weights_grad[layer_num] + (1 - args.grad_smoothing)*dC_by_softmax_weights/args.batch_size\n",
    "            softmax_weights[layer_num] += weight_mult*args.lr_softmax*(softmax_weights_grad[layer_num] - args.wc_softmax*softmax_weights[layer_num])\n",
    "        # Make Negative Data\n",
    "        neg_data = x\n",
    "        # Big negative logits\n",
    "        label_in_others = label_in - 1000*y\n",
    "        neg_data[:, :args.num_classes] = args.label_strength*choose_from_probs(np.exp(label_in_others)/sum(np.exp(label_in_others)))\n",
    "        normed_states[-1] = norm_rows(neg_data)\n",
    "        for layer_num in range(args.num_layers):\n",
    "            net_inputs[layer_num] = normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num]\n",
    "            relu_output = np.maximum(net_inputs[layer_num], 0)\n",
    "            hidden_states[layer_num] = relu_output\n",
    "            normed_states[layer_num] = norm_rows(relu_output)\n",
    "            goodness = np.sum(hidden_states[layer_num]**2, axis=1, keepdims=True)\n",
    "            # Really not sure why we are subtracting this\n",
    "            neg_probs[layer_num] = (logistic_fn(goodness - (args.layer_dims[layer_num + 1]/args.temp)))\n",
    "            dC_by_din[layer_num] = np.tile(-1*neg_probs[layer_num], (1, args.layer_dims[layer_num + 1]))*relu_output\n",
    "            neg_ex_dC_by_dweights[layer_num] = normed_states[layer_num - 1].T @ dC_by_din[layer_num]\n",
    "            neg_ex_dC_by_dbiases[layer_num] = np.sum(dC_by_din[layer_num], axis=0, keepdims=True)\n",
    "            pair_sum_errors[layer_num] += np.sum(neg_probs[layer_num] > pos_probs[layer_num])\n",
    "        # Gradient Backprop (FINALLY)\n",
    "        for layer_num in range(args.num_layers):\n",
    "            dC_by_dW = (pos_ex_dC_by_dweights[layer_num] + neg_ex_dC_by_dweights[layer_num])/args.batch_size\n",
    "            weights_grad[layer_num] = args.grad_smoothing*weights_grad[layer_num] + (1 - args.grad_smoothing)*(dC_by_dW)\n",
    "            \n",
    "            dC_by_dB = (pos_ex_dC_by_dbiases[layer_num] + neg_ex_dC_by_dbiases[layer_num])/args.batch_size\n",
    "            biases_grad[layer_num] = args.grad_smoothing*biases_grad[layer_num] + (1 - args.grad_smoothing)*dC_by_dB\n",
    "            biases[layer_num] += args.lr_hidden*weight_mult*biases_grad[layer_num]\n",
    "            weights[layer_num] += args.lr_hidden*weight_mult*(weights_grad[layer_num] - args.wc_hidden*weights[layer_num])\n",
    "\n",
    "    print(f\"Train Errors Epoch {epoch} = {train_errors}\")\n",
    "    print(f\"Accuracy = {100.0*(args.trainset_size-train_errors)/(1.0*args.trainset_size)}%\")\n",
    "print(f\"Donesies Train Errors = {train_errors}\")\n",
    "print(f\"Accuracy = {100.0*(args.trainset_size-train_errors)/(1.0*args.trainset_size)}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9x/93y24nln7mjb1qs1mnw_0_f00000gn/T/ipykernel_32448/4261151372.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatdat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'finaltestbatchtargets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# test_errors, num_test_data, _ = calc_forward_errors(batch_data=test_x, batch_targets=test_y, args=args)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mtest_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_test_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_softmax_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/9x/93y24nln7mjb1qs1mnw_0_f00000gn/T/ipykernel_32448/4261151372.py\u001b[0m in \u001b[0;36mcalc_softmax_errors\u001b[0;34m(x_batch, y_batch, args)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormed_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_num\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mnormed_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mlabel_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "def calc_forward_errors(x_batch, y_batch, args):\n",
    "    normed_states = {}\n",
    "    hidden_states = {}\n",
    "    test_batch_size, _, num_batches = x_batch.shape\n",
    "    num_test_data = test_batch_size*num_batches\n",
    "    num_errors = 0\n",
    "    for batch_idx in range(num_batches):\n",
    "        x = x_batch[:, :, batch_idx]\n",
    "        y = y_batch[:, :, batch_idx]\n",
    "        act_sum_sqrs = np.zeros([num_batches, args.num_classes])\n",
    "        for label_num in range(args.num_classes):\n",
    "            x[:, :args.num_classes] = np.zeros([num_batches, args.num_classes])\n",
    "            x[:, label_num] = args.label_strength * np.ones([num_batches])\n",
    "            normed_states[-1] = norm_rows(x)\n",
    "            for layer_num in range(args.num_layers):\n",
    "                hidden_states[layer_num] = np.maximum(0, normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num])\n",
    "                if layer_num >= args.min_level_energy:\n",
    "                    act_sum_sqrs[:, label_num] += np.sum(hidden_states[layer_num]**2, axis=1)\n",
    "                normed_states[layer_num] = norm_rows(hidden_states[layer_num])\n",
    "        guesses = np.argmax(act_sum_sqrs, axis=1)\n",
    "        targets_argmax = np.argmax(y, axis=1)\n",
    "        num_errors += np.sum(guesses != targets_argmax)\n",
    "    return num_errors, num_test_data\n",
    "\n",
    "def calc_softmax_errors(x_batch, y_batch, args):\n",
    "    normed_states = {}\n",
    "    test_batch_size, _, num_batches = x_batch.shape\n",
    "    num_test_data = test_batch_size*num_batches\n",
    "    num_errors = 0\n",
    "    log_cost = 0\n",
    "    for batch_idx in range(num_batches):\n",
    "        x = x_batch[:, :, batch_idx]\n",
    "        x[:, :args.num_classes] = args.label_strength * np.ones([test_batch_size, args.num_classes])/args.num_classes\n",
    "        y = y_batch[:, :, batch_idx]\n",
    "        normed_states[-1] = norm_rows(x)\n",
    "        for layer_num in range(args.num_layers):\n",
    "            hidden_state = np.maximum(0, normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num])\n",
    "            normed_states[layer_num] = norm_rows(hidden_state)\n",
    "        label_in = np.tile(biases[args.num_layers], (args.num_classes, 1))\n",
    "        # Now do the forward pass using the labels from here\n",
    "        for layer_num in range(args.min_layer_softmax, args.num_layers):\n",
    "            label_in += normed_states[layer_num] @ softmax_weights[layer_num]\n",
    "        label_in -= np.tile(np.max(label_in, axis=1, keepdims=True), (1, args.num_classes))\n",
    "        # Can probably replace this with a softmax:\n",
    "        unnorm_probs = np.exp(label_in)\n",
    "        preds = unnorm_probs/np.tile(np.sum(unnorm_probs, axis=1, keepdims=True), (1, args.num_classes))\n",
    "        guesses = np.argmax(preds, axis=1)\n",
    "        targets_argmax = np.argmax(y, axis=1)\n",
    "        num_errors += np.sum(guesses != targets_argmax)\n",
    "        # Calculate the\n",
    "        log_costs_batch = -1*np.sum(np.sum(y * np.log(args.tiny + preds), axis=1, keepdims=True), axis=1, keepdims=True)\n",
    "        log_cost += np.sum(log_costs_batch)/num_batches\n",
    "    return num_errors, num_test_data, log_cost\n",
    "\n",
    "\n",
    "test_x = matdat['finaltestbatchdata'].astype(args.dtype, copy=False)\n",
    "test_y = matdat['finaltestbatchtargets'].astype(args.dtype, copy=False)\n",
    "# test_errors, num_test_data, _ = calc_forward_errors(batch_data=test_x, batch_targets=test_y, args=args)\n",
    "test_errors, num_test_data, _ = calc_softmax_errors(x_batch=test_x, y_batch=test_y, args=args)\n",
    "\n",
    "\n",
    "print(f\"Number of testing errors = {test_errors}\")\n",
    "print(f\"Test error percentage = {test_errors*100.0/num_test_data}\")\n",
    "print(f\"Test Accuracy = {100 - (test_errors*100.0/num_test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Viewing the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(x[3, :].reshape(28, 28), cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04ffba0d41b4216980bf6c7d893a68b19ea460a3309261abb88c05c7a783f5c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
