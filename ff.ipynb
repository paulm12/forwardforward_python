{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "matdat = loadmat(\"mnistdata.mat\")\n",
    "batch_data = matdat[\"batchdata\"]\n",
    "batch_targets = matdat[\"batchtargets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, d_in, num_batches = batch_data.shape\n",
    "_, num_classes, _= batch_targets.shape\n",
    "\n",
    "class Args():\n",
    "    d_in = d_in\n",
    "    batch_size = batch_size\n",
    "    num_batches = num_batches\n",
    "    trainset_size = batch_size*num_batches\n",
    "    num_classes = num_classes\n",
    "    label_strength = 1 # Hinton's \"labelstrength\", the strength of the layer pixel\n",
    "    min_layer_softmax = 1 # Does not use hidden layers lower than this\n",
    "    min_layer_energy = 1 # Used for computing goodness at test time\n",
    "    wc_hidden = 0.001   # Hinton's \"wc\" for forward weights\n",
    "    wc_softmax = 0.003 # Hinton's \"sup_wc\" for label prediction\n",
    "    lr_hidden = 0.01 # Hinton's \"epsilon\"\n",
    "    lr_softmax = 0.1 # Hinton's \"epsilonup\"\n",
    "    grad_smoothing = 0.9 # Hinton's \"delay\" variable, default=0.9\n",
    "    lambda_mean = 0.03 # Peer normalization (WTF is this?)\n",
    "    num_epochs = 100    # Maximum number of epochs; Hinton uses 100\n",
    "    layer_dims = [d_in, 1000, 1000, 1000, num_classes] # Layer sizes for the model (always start with d_in and end with num_lables)\n",
    "    num_layers = len(layer_dims) - 1\n",
    "    temp = 1 # Used for rescaling weights (Not used)\n",
    "    tiny = np.exp(-50)\n",
    "    dtype=np.float32 # Was trying to speed this up, doesn't help much\n",
    "args = Args()\n",
    "\n",
    "def norm_rows(x):\n",
    "    # Makes the sum of squared activations be 1 per neuron\n",
    "    eps = np.exp(-100,  dtype=args.dtype)\n",
    "    num_comp = x.shape[1]\n",
    "    # Considering using vectors of ones instead\n",
    "    # Need to use keepdims for broadcasting\n",
    "    return x / np.tile(eps + np.mean(x ** 2, axis=1, keepdims=True)**(0.5), (1, num_comp))\n",
    "\n",
    "def logistic_fn(x):\n",
    "    return np.divide(1, 1 + np.exp(-1*x))\n",
    "\n",
    "\n",
    "def choose_from_probs(probs):\n",
    "    batch_size, num_labs = probs.shape\n",
    "    rands = np.random.rand(batch_size)\n",
    "    choices = np.zeros(probs.shape, dtype=args.dtype)\n",
    "    for n in range(batch_size):\n",
    "        sum_so_far = 0\n",
    "        used = 0\n",
    "        for lab_idx in range(num_labs):\n",
    "            sum_so_far += probs[n, lab_idx]\n",
    "            if rands[n] < sum_so_far and used == 0:\n",
    "                used = 1\n",
    "                choices[n, lab_idx] = 1\n",
    "                break\n",
    "    return choices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done setting everything up\n"
     ]
    }
   ],
   "source": [
    "net_inputs = {}\n",
    "hidden_states = {}\n",
    "normed_states = {}\n",
    "pos_probs = {}\n",
    "neg_probs = {}\n",
    "weights = {}\n",
    "weights_grad = {}\n",
    "biases = {}\n",
    "biases_grad = {}\n",
    "dC_by_din = {}\n",
    "pos_ex_dC_by_dweights = {}\n",
    "neg_ex_dC_by_dweights = {}\n",
    "pos_ex_dC_by_dbiases = {}\n",
    "neg_ex_dC_by_dbiases = {}\n",
    "mean_states = {}  # Running average of hidden layers\n",
    "softmax_weights = {}\n",
    "softmax_weights_grad = {}\n",
    "\n",
    "# Initialization Loop:\n",
    "for layer_num in range(args.num_layers):\n",
    "    d_in = args.layer_dims[layer_num]\n",
    "    d_out = args.layer_dims[layer_num + 1]\n",
    "    net_inputs[layer_num] = np.zeros([args.batch_size, d_out], dtype=args.dtype)\n",
    "    # Keeping track of all the input stuff at each layer\n",
    "    hidden_states[layer_num] = np.zeros([args.batch_size, d_out], dtype=args.dtype)\n",
    "    normed_states[layer_num] = np.zeros([args.batch_size, d_out], dtype=args.dtype)\n",
    "    pos_probs[layer_num] = np.zeros(args.batch_size, dtype=args.dtype)\n",
    "    neg_probs[layer_num] = np.zeros(args.batch_size, dtype=args.dtype)\n",
    "    # Keep track of the network information:\n",
    "    weights[layer_num] = (1/np.sqrt(d_in,  dtype=args.dtype) * np.random.randn(d_in, d_out))\n",
    "    weights_grad[layer_num] = np.zeros([d_in, d_out], dtype=args.dtype)\n",
    "    biases[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    biases_grad[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    pos_ex_dC_by_dweights[layer_num] = np.zeros([d_in, d_out], dtype=args.dtype)\n",
    "    neg_ex_dC_by_dweights[layer_num] = np.zeros([d_in, d_out], dtype=args.dtype)\n",
    "    pos_ex_dC_by_dbiases[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    neg_ex_dC_by_dbiases[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    mean_states[layer_num] = 0.5*np.ones([1, d_out], dtype=args.dtype)\n",
    "    softmax_weights[layer_num] = np.zeros([d_out, args.num_classes], dtype=args.dtype)\n",
    "    softmax_weights_grad[layer_num] = np.zeros([d_out, args.num_classes], dtype=args.dtype)\n",
    "\n",
    "batch_data.astype(args.dtype, copy=False)\n",
    "batch_targets.astype(args.dtype, copy=False)\n",
    "print(\"Done setting everything up\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nz/r991tz5d1n1fdby4l9mhm76w0000gn/T/ipykernel_2374/955794127.py:36: RuntimeWarning: overflow encountered in exp\n",
      "  return np.divide(1, 1 + np.exp(-1*x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Errors Epoch 0 = 9410\n",
      "Accuracy = 81.18%\n",
      "Train Errors Epoch 1 = 5149\n",
      "Accuracy = 89.702%\n",
      "Train Errors Epoch 2 = 4619\n",
      "Accuracy = 90.762%\n",
      "Train Errors Epoch 3 = 3582\n",
      "Accuracy = 92.836%\n",
      "Train Errors Epoch 4 = 3069\n",
      "Accuracy = 93.862%\n",
      "Train Errors Epoch 5 = 3003\n",
      "Accuracy = 93.994%\n",
      "Train Errors Epoch 6 = 2690\n",
      "Accuracy = 94.62%\n",
      "Train Errors Epoch 7 = 2193\n",
      "Accuracy = 95.614%\n",
      "Train Errors Epoch 8 = 2056\n",
      "Accuracy = 95.888%\n",
      "Train Errors Epoch 9 = 1952\n",
      "Accuracy = 96.096%\n",
      "Train Errors Epoch 10 = 1918\n",
      "Accuracy = 96.164%\n",
      "Train Errors Epoch 11 = 1884\n",
      "Accuracy = 96.232%\n",
      "Train Errors Epoch 12 = 1705\n",
      "Accuracy = 96.59%\n",
      "Train Errors Epoch 13 = 1428\n",
      "Accuracy = 97.144%\n",
      "Train Errors Epoch 14 = 1412\n",
      "Accuracy = 97.176%\n",
      "Train Errors Epoch 15 = 1517\n",
      "Accuracy = 96.966%\n",
      "Train Errors Epoch 16 = 1352\n",
      "Accuracy = 97.296%\n",
      "Train Errors Epoch 17 = 1287\n",
      "Accuracy = 97.426%\n",
      "Train Errors Epoch 18 = 1021\n",
      "Accuracy = 97.958%\n",
      "Train Errors Epoch 19 = 1143\n",
      "Accuracy = 97.714%\n",
      "Train Errors Epoch 20 = 1151\n",
      "Accuracy = 97.698%\n",
      "Train Errors Epoch 21 = 1044\n",
      "Accuracy = 97.912%\n",
      "Train Errors Epoch 22 = 1008\n",
      "Accuracy = 97.984%\n",
      "Train Errors Epoch 23 = 1016\n",
      "Accuracy = 97.968%\n",
      "Train Errors Epoch 24 = 837\n",
      "Accuracy = 98.326%\n",
      "Train Errors Epoch 25 = 840\n",
      "Accuracy = 98.32%\n",
      "Train Errors Epoch 26 = 788\n",
      "Accuracy = 98.424%\n",
      "Train Errors Epoch 27 = 854\n",
      "Accuracy = 98.292%\n",
      "Train Errors Epoch 28 = 772\n",
      "Accuracy = 98.456%\n",
      "Train Errors Epoch 29 = 784\n",
      "Accuracy = 98.432%\n",
      "Train Errors Epoch 30 = 806\n",
      "Accuracy = 98.388%\n",
      "Train Errors Epoch 31 = 664\n",
      "Accuracy = 98.672%\n",
      "Train Errors Epoch 32 = 738\n",
      "Accuracy = 98.524%\n",
      "Train Errors Epoch 33 = 619\n",
      "Accuracy = 98.762%\n",
      "Train Errors Epoch 34 = 580\n",
      "Accuracy = 98.84%\n",
      "Train Errors Epoch 35 = 624\n",
      "Accuracy = 98.752%\n",
      "Train Errors Epoch 36 = 569\n",
      "Accuracy = 98.862%\n",
      "Train Errors Epoch 37 = 644\n",
      "Accuracy = 98.712%\n",
      "Train Errors Epoch 38 = 526\n",
      "Accuracy = 98.948%\n",
      "Train Errors Epoch 39 = 555\n",
      "Accuracy = 98.89%\n",
      "Train Errors Epoch 40 = 521\n",
      "Accuracy = 98.958%\n",
      "Train Errors Epoch 41 = 540\n",
      "Accuracy = 98.92%\n",
      "Train Errors Epoch 42 = 458\n",
      "Accuracy = 99.084%\n",
      "Train Errors Epoch 43 = 411\n",
      "Accuracy = 99.178%\n",
      "Train Errors Epoch 44 = 488\n",
      "Accuracy = 99.024%\n",
      "Train Errors Epoch 45 = 449\n",
      "Accuracy = 99.102%\n",
      "Train Errors Epoch 46 = 477\n",
      "Accuracy = 99.046%\n",
      "Train Errors Epoch 47 = 400\n",
      "Accuracy = 99.2%\n",
      "Train Errors Epoch 48 = 344\n",
      "Accuracy = 99.312%\n",
      "Train Errors Epoch 49 = 289\n",
      "Accuracy = 99.422%\n",
      "Train Errors Epoch 50 = 351\n",
      "Accuracy = 99.298%\n",
      "Train Errors Epoch 51 = 345\n",
      "Accuracy = 99.31%\n",
      "Train Errors Epoch 52 = 320\n",
      "Accuracy = 99.36%\n",
      "Train Errors Epoch 53 = 295\n",
      "Accuracy = 99.41%\n",
      "Train Errors Epoch 54 = 278\n",
      "Accuracy = 99.444%\n",
      "Train Errors Epoch 55 = 221\n",
      "Accuracy = 99.558%\n",
      "Train Errors Epoch 56 = 251\n",
      "Accuracy = 99.498%\n",
      "Train Errors Epoch 57 = 187\n",
      "Accuracy = 99.626%\n",
      "Train Errors Epoch 58 = 186\n",
      "Accuracy = 99.628%\n",
      "Train Errors Epoch 59 = 209\n",
      "Accuracy = 99.582%\n",
      "Train Errors Epoch 60 = 181\n",
      "Accuracy = 99.638%\n",
      "Train Errors Epoch 61 = 147\n",
      "Accuracy = 99.706%\n",
      "Train Errors Epoch 62 = 129\n",
      "Accuracy = 99.742%\n",
      "Train Errors Epoch 63 = 122\n",
      "Accuracy = 99.756%\n",
      "Train Errors Epoch 64 = 128\n",
      "Accuracy = 99.744%\n",
      "Train Errors Epoch 65 = 106\n",
      "Accuracy = 99.788%\n",
      "Train Errors Epoch 66 = 88\n",
      "Accuracy = 99.824%\n",
      "Train Errors Epoch 67 = 83\n",
      "Accuracy = 99.834%\n",
      "Train Errors Epoch 68 = 62\n",
      "Accuracy = 99.876%\n",
      "Train Errors Epoch 69 = 70\n",
      "Accuracy = 99.86%\n",
      "Train Errors Epoch 70 = 60\n",
      "Accuracy = 99.88%\n",
      "Train Errors Epoch 71 = 44\n",
      "Accuracy = 99.912%\n",
      "Train Errors Epoch 72 = 46\n",
      "Accuracy = 99.908%\n",
      "Train Errors Epoch 73 = 33\n",
      "Accuracy = 99.934%\n",
      "Train Errors Epoch 74 = 37\n",
      "Accuracy = 99.926%\n",
      "Train Errors Epoch 75 = 34\n",
      "Accuracy = 99.932%\n",
      "Train Errors Epoch 76 = 34\n",
      "Accuracy = 99.932%\n",
      "Train Errors Epoch 77 = 31\n",
      "Accuracy = 99.938%\n",
      "Train Errors Epoch 78 = 26\n",
      "Accuracy = 99.948%\n",
      "Train Errors Epoch 79 = 16\n",
      "Accuracy = 99.968%\n",
      "Train Errors Epoch 80 = 19\n",
      "Accuracy = 99.962%\n",
      "Train Errors Epoch 81 = 10\n",
      "Accuracy = 99.98%\n",
      "Train Errors Epoch 82 = 12\n",
      "Accuracy = 99.976%\n",
      "Train Errors Epoch 83 = 9\n",
      "Accuracy = 99.982%\n",
      "Train Errors Epoch 84 = 12\n",
      "Accuracy = 99.976%\n",
      "Train Errors Epoch 85 = 10\n",
      "Accuracy = 99.98%\n",
      "Train Errors Epoch 86 = 14\n",
      "Accuracy = 99.972%\n",
      "Train Errors Epoch 87 = 12\n",
      "Accuracy = 99.976%\n",
      "Train Errors Epoch 88 = 8\n",
      "Accuracy = 99.984%\n",
      "Train Errors Epoch 89 = 10\n",
      "Accuracy = 99.98%\n",
      "Train Errors Epoch 90 = 6\n",
      "Accuracy = 99.988%\n",
      "Train Errors Epoch 91 = 8\n",
      "Accuracy = 99.984%\n",
      "Train Errors Epoch 92 = 6\n",
      "Accuracy = 99.988%\n",
      "Train Errors Epoch 93 = 4\n",
      "Accuracy = 99.992%\n",
      "Train Errors Epoch 94 = 5\n",
      "Accuracy = 99.99%\n",
      "Train Errors Epoch 95 = 5\n",
      "Accuracy = 99.99%\n",
      "Train Errors Epoch 96 = 3\n",
      "Accuracy = 99.994%\n",
      "Train Errors Epoch 97 = 4\n",
      "Accuracy = 99.992%\n",
      "Train Errors Epoch 98 = 4\n",
      "Accuracy = 99.992%\n",
      "Train Errors Epoch 99 = 4\n",
      "Accuracy = 99.992%\n",
      "Donesies Train Errors = 4\n",
      "Accuracy = 99.992%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.num_epochs):\n",
    "    pos_errors = np.zeros(args.num_layers, dtype=args.dtype)\n",
    "    neg_errors = np.zeros(args.num_layers, dtype=args.dtype)\n",
    "    pair_sum_errors = np.zeros(args.num_layers, dtype=args.dtype)\n",
    "    train_log_cost = 0\n",
    "    train_errors = 0\n",
    "    #  Linear decaying lr after first half of training; Hinton's \"epsgain\" parameter\n",
    "    if epoch < args.num_epochs/2.0:\n",
    "        weight_mult = 1 # This is Hinton's \"epsgain\" parameter\n",
    "    else:\n",
    "        weight_mult = (1 + 2.0*(args.num_epochs - epoch))/args.num_epochs\n",
    "    for batch in range(args.num_batches):\n",
    "        x = batch_data[:, :, batch]\n",
    "        y = batch_targets[:, :, batch]\n",
    "        # Now add the target label to the image (in the first few pixels)\n",
    "        x[:, 0:args.num_classes] = args.label_strength * y\n",
    "        normed_states[-1] = norm_rows(x)\n",
    "        for layer_num in range(args.num_layers):\n",
    "            # Forward Pass for each layer\n",
    "            net_inputs[layer_num] = normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num]\n",
    "            # ReLU\n",
    "            relu_output = np.maximum(net_inputs[layer_num], 0)\n",
    "            hidden_states[layer_num] = relu_output\n",
    "            normed_states[layer_num] = norm_rows(hidden_states[layer_num])\n",
    "            goodness = np.sum(hidden_states[layer_num]**2, axis=1, keepdims=True)\n",
    "            # Really not sure why we are subtracting this\n",
    "            pos_probs[layer_num] = (logistic_fn(goodness - (args.layer_dims[layer_num + 1]/args.temp)))\n",
    "            dC_by_din[layer_num] = np.tile(1 - pos_probs[layer_num], (1, args.layer_dims[layer_num + 1]))*relu_output\n",
    "            mean_states[layer_num] = 0.9*mean_states[layer_num] + 0.1*np.mean(relu_output, axis=0, keepdims=True)\n",
    "            # Regularizer encouraging layers to turn on\n",
    "            dC_by_din[layer_num] += args.lambda_mean*(np.mean(mean_states[layer_num]) - mean_states[layer_num])\n",
    "            pos_ex_dC_by_dweights[layer_num] = normed_states[layer_num - 1].T @ dC_by_din[layer_num]\n",
    "            pos_ex_dC_by_dbiases[layer_num] = np.sum(dC_by_din[layer_num], axis=0, keepdims=True)\n",
    "        # Now get hidden states when label is neutral.  Use this to pick hard negative labels\n",
    "        x[:, 0:args.num_classes] = args.label_strength * np.ones([args.batch_size, args.num_classes], dtype=args.dtype)/args.num_classes\n",
    "        normed_states[-1] = norm_rows(x)\n",
    "        # Run the neutral data through the forward pass\n",
    "        for layer_num in range(args.num_layers):\n",
    "            net_inputs[layer_num] = normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num]\n",
    "            relu_output = np.maximum(net_inputs[layer_num], 0)\n",
    "            hidden_states[layer_num] = relu_output\n",
    "            normed_states[layer_num] = norm_rows(relu_output)\n",
    "        # Looks like we use the last layer to calculate the negative data, some sort of feedback\n",
    "        label_in = np.tile(biases[args.num_layers - 1], (args.batch_size, 1))\n",
    "        for layer_num in range(args.min_layer_softmax, args.num_layers):\n",
    "            label_in += normed_states[layer_num] @ softmax_weights[layer_num]\n",
    "        label_in = label_in - np.tile(np.max(label_in, axis=1, keepdims=True), (1, args.num_classes))\n",
    "        unnorm_probs = np.exp(label_in)\n",
    "        train_predictions = unnorm_probs/np.tile(np.sum(unnorm_probs, axis=1, keepdims=True), (1, args.num_classes))\n",
    "        correct_probs = np.sum(train_predictions*y, axis=1, keepdims=True) # Should be a Column vector\n",
    "        curr_train_log_cost = -1 * np.log(args.tiny + correct_probs)\n",
    "        train_log_cost += np.sum(curr_train_log_cost)/args.num_batches\n",
    "        train_guesses = np.argmax(train_predictions, axis=1)\n",
    "        target_indices = np.argmax(y, axis=1)\n",
    "        train_errors += np.sum(train_guesses != target_indices)\n",
    "        # Now do the backprop step\n",
    "        dC_by_din[args.num_layers] = y - train_predictions\n",
    "        # Not used:\n",
    "        # dC_by_dbiases = sum(dC_by_din[args.num_layers], axis=0)\n",
    "        for layer_num in range(args.min_layer_softmax, args.num_layers):\n",
    "            dC_by_softmax_weights = normed_states[layer_num].T @ dC_by_din[args.num_layers]\n",
    "            softmax_weights_grad[layer_num] = args.grad_smoothing * softmax_weights_grad[layer_num] + (1 - args.grad_smoothing)*dC_by_softmax_weights/args.batch_size\n",
    "            softmax_weights[layer_num] += weight_mult*args.lr_softmax*(softmax_weights_grad[layer_num] - args.wc_softmax*softmax_weights[layer_num])\n",
    "        # Make Negative Data\n",
    "        neg_data = x\n",
    "        # Big negative logits\n",
    "        label_in_others = label_in - 1000*y\n",
    "        neg_data[:, :args.num_classes] = args.label_strength*choose_from_probs(np.exp(label_in_others)/sum(np.exp(label_in_others)))\n",
    "        normed_states[-1] = norm_rows(neg_data)\n",
    "        for layer_num in range(args.num_layers):\n",
    "            net_inputs[layer_num] = normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num]\n",
    "            relu_output = np.maximum(net_inputs[layer_num], 0)\n",
    "            hidden_states[layer_num] = relu_output\n",
    "            normed_states[layer_num] = norm_rows(relu_output)\n",
    "            goodness = np.sum(hidden_states[layer_num]**2, axis=1, keepdims=True)\n",
    "            # Really not sure why we are subtracting this\n",
    "            neg_probs[layer_num] = (logistic_fn(goodness - (args.layer_dims[layer_num + 1]/args.temp)))\n",
    "            dC_by_din[layer_num] = np.tile(-1*neg_probs[layer_num], (1, args.layer_dims[layer_num + 1]))*relu_output\n",
    "            neg_ex_dC_by_dweights[layer_num] = normed_states[layer_num - 1].T @ dC_by_din[layer_num]\n",
    "            neg_ex_dC_by_dbiases[layer_num] = np.sum(dC_by_din[layer_num], axis=0, keepdims=True)\n",
    "            pair_sum_errors[layer_num] += np.sum(neg_probs[layer_num] > pos_probs[layer_num])\n",
    "        # Gradient Backprop (FINALLY)\n",
    "        for layer_num in range(args.num_layers):\n",
    "            dC_by_dW = (pos_ex_dC_by_dweights[layer_num] + neg_ex_dC_by_dweights[layer_num])/args.batch_size\n",
    "            weights_grad[layer_num] = args.grad_smoothing*weights_grad[layer_num] + (1 - args.grad_smoothing)*(dC_by_dW)\n",
    "            \n",
    "            dC_by_dB = (pos_ex_dC_by_dbiases[layer_num] + neg_ex_dC_by_dbiases[layer_num])/args.batch_size\n",
    "            biases_grad[layer_num] = args.grad_smoothing*biases_grad[layer_num] + (1 - args.grad_smoothing)*dC_by_dB\n",
    "            biases[layer_num] += args.lr_hidden*weight_mult*biases_grad[layer_num]\n",
    "            weights[layer_num] += args.lr_hidden*weight_mult*(weights_grad[layer_num] - args.wc_hidden*weights[layer_num])\n",
    "\n",
    "    print(f\"Train Errors Epoch {epoch} = {train_errors}\")\n",
    "    print(f\"Accuracy = {100.0*(args.trainset_size-train_errors)/(1.0*args.trainset_size)}%\")\n",
    "print(f\"Donesies Train Errors = {train_errors}\")\n",
    "print(f\"Accuracy = {100.0*(args.trainset_size-train_errors)/(1.0*args.trainset_size)}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing errors = 167\n",
      "Test error percentage = 1.67\n",
      "Test Accuracy = 98.33\n"
     ]
    }
   ],
   "source": [
    "def calc_forward_errors(x_batch, y_batch, args):\n",
    "    normed_states = {}\n",
    "    hidden_states = {}\n",
    "    test_batch_size, _, num_batches = x_batch.shape\n",
    "    num_test_data = test_batch_size*num_batches\n",
    "    num_errors = 0\n",
    "    for batch_idx in range(num_batches):\n",
    "        x = x_batch[:, :, batch_idx]\n",
    "        y = y_batch[:, :, batch_idx]\n",
    "        act_sum_sqrs = np.zeros([num_batches, args.num_classes])\n",
    "        for label_num in range(args.num_classes):\n",
    "            x[:, :args.num_classes] = np.zeros([num_batches, args.num_classes])\n",
    "            x[:, label_num] = args.label_strength * np.ones([num_batches])\n",
    "            normed_states[-1] = norm_rows(x)\n",
    "            for layer_num in range(args.num_layers):\n",
    "                hidden_states[layer_num] = np.maximum(0, normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num])\n",
    "                if layer_num >= args.min_layer_energy:\n",
    "                    act_sum_sqrs[:, label_num] += np.sum(hidden_states[layer_num]**2, axis=1)\n",
    "                normed_states[layer_num] = norm_rows(hidden_states[layer_num])\n",
    "        guesses = np.argmax(act_sum_sqrs, axis=1)\n",
    "        targets_argmax = np.argmax(y, axis=1)\n",
    "        num_errors += np.sum(guesses != targets_argmax)\n",
    "    return num_errors, num_test_data\n",
    "\n",
    "def calc_softmax_errors(x_batch, y_batch, args):\n",
    "    normed_states = {}\n",
    "    test_batch_size, _, num_batches = x_batch.shape\n",
    "    num_test_data = test_batch_size*num_batches\n",
    "    num_errors = 0\n",
    "    log_cost = 0\n",
    "    for batch_idx in range(num_batches):\n",
    "        x = x_batch[:, :, batch_idx]\n",
    "        x[:, :args.num_classes] = args.label_strength * np.ones([test_batch_size, args.num_classes])/args.num_classes\n",
    "        y = y_batch[:, :, batch_idx]\n",
    "        normed_states[-1] = norm_rows(x)\n",
    "        for layer_num in range(args.num_layers):\n",
    "            hidden_state = np.maximum(0, normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num])\n",
    "            normed_states[layer_num] = norm_rows(hidden_state)\n",
    "        label_in = np.tile(biases[args.num_layers - 1], (test_batch_size, 1))\n",
    "        # Now do the forward pass using the labels from here\n",
    "        for layer_num in range(args.min_layer_softmax, args.num_layers):\n",
    "            label_in += normed_states[layer_num] @ softmax_weights[layer_num]\n",
    "        label_in -= np.tile(np.max(label_in, axis=1, keepdims=True), (1, args.num_classes))\n",
    "        # Can probably replace this with a softmax:\n",
    "        unnorm_probs = np.exp(label_in)\n",
    "        preds = unnorm_probs/np.tile(np.sum(unnorm_probs, axis=1, keepdims=True), (1, args.num_classes))\n",
    "        guesses = np.argmax(preds, axis=1)\n",
    "        targets_argmax = np.argmax(y, axis=1)\n",
    "        num_errors += np.sum(guesses != targets_argmax)\n",
    "        # Calculate the\n",
    "        log_costs_batch = -1*np.sum(np.sum(y * np.log(args.tiny + preds), axis=1, keepdims=True), axis=1, keepdims=True)\n",
    "        log_cost += np.sum(log_costs_batch)/num_batches\n",
    "    return num_errors, num_test_data, log_cost\n",
    "\n",
    "\n",
    "test_x = matdat['finaltestbatchdata'].astype(args.dtype, copy=False)\n",
    "test_y = matdat['finaltestbatchtargets'].astype(args.dtype, copy=False)\n",
    "# test_errors, num_test_data, = calc_forward_errors(x_batch=test_x, y_batch=test_y, args=args)\n",
    "test_errors, num_test_data, _ = calc_softmax_errors(x_batch=test_x, y_batch=test_y, args=args)\n",
    "\n",
    "\n",
    "print(f\"Number of testing errors = {test_errors}\")\n",
    "print(f\"Test error percentage = {test_errors*100.0/num_test_data}\")\n",
    "print(f\"Test Accuracy = {100 - (test_errors*100.0/num_test_data)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Viewing the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x[3, :].reshape(28, 28), cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04ffba0d41b4216980bf6c7d893a68b19ea460a3309261abb88c05c7a783f5c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
