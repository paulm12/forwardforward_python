{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "matdat = loadmat(\"mnistdata.mat\")\n",
    "batch_data = matdat[\"batchdata\"]\n",
    "batch_targets = matdat[\"batchtargets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, d_in, num_batches = batch_data.shape\n",
    "_, num_classes, _= batch_targets.shape\n",
    "\n",
    "class Args():\n",
    "    d_in = d_in\n",
    "    batch_size = batch_size\n",
    "    num_batches = num_batches\n",
    "    trainset_size = batch_size*num_batches\n",
    "    num_classes = num_classes\n",
    "    label_strength = 1 # Hinton's \"labelstrength\", the strength of the layer pixel\n",
    "    min_layer_softmax = 1 # Does not use hidden layers lower than this\n",
    "    min_layer_energy = 1 # Used for computing goodness at test time\n",
    "    wc_hidden = 0.001   # Hinton's \"wc\" for forward weights\n",
    "    wc_softmax = 0.003 # Hinton's \"sup_wc\" for label prediction\n",
    "    lr_hidden = 0.01 # Hinton's \"epsilon\"\n",
    "    lr_softmax = 0.1 # Hinton's \"epsilonup\"\n",
    "    grad_smoothing = 0.9 # Hinton's \"delay\" variable, default=0.9\n",
    "    lambda_mean = 0.03 # Peer normalization (WTF is this?)\n",
    "    num_epochs = 100    # Maximum number of epochs; Hinton uses 100\n",
    "    layer_dims = [d_in, 1000, 1000, 1000, num_classes] # Layer sizes for the model (always start with d_in and end with num_lables)\n",
    "    num_layers = len(layer_dims) - 1\n",
    "    temp = 1 # Used for rescaling weights (Not used)\n",
    "    tiny = np.exp(-50)\n",
    "    dtype=np.float32 # Was trying to speed this up, doesn't help much\n",
    "args = Args()\n",
    "\n",
    "def norm_rows(x):\n",
    "    # Makes the sum of squared activations be 1 per neuron\n",
    "    eps = np.exp(-100,  dtype=args.dtype)\n",
    "    num_comp = x.shape[1]\n",
    "    # Considering using vectors of ones instead\n",
    "    # Need to use keepdims for broadcasting\n",
    "    return x / np.tile(eps + np.mean(x ** 2, axis=1, keepdims=True)**(0.5), (1, num_comp))\n",
    "\n",
    "def logistic_fn(x):\n",
    "    return np.divide(1, 1 + np.exp(-1*x))\n",
    "\n",
    "\n",
    "def choose_from_probs(probs):\n",
    "    batch_size, num_labs = probs.shape\n",
    "    rands = np.random.rand(batch_size)\n",
    "    choices = np.zeros(probs.shape, dtype=args.dtype)\n",
    "    for n in range(batch_size):\n",
    "        sum_so_far = 0\n",
    "        used = 0\n",
    "        for lab_idx in range(num_labs):\n",
    "            sum_so_far += probs[n, lab_idx]\n",
    "            if rands[n] < sum_so_far and used == 0:\n",
    "                used = 1\n",
    "                choices[n, lab_idx] = 1\n",
    "                break\n",
    "    return choices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done setting everything up\n"
     ]
    }
   ],
   "source": [
    "net_inputs = {}\n",
    "hidden_states = {}\n",
    "normed_states = {}\n",
    "pos_probs = {}\n",
    "neg_probs = {}\n",
    "weights = {}\n",
    "weights_grad = {}\n",
    "biases = {}\n",
    "biases_grad = {}\n",
    "dC_by_din = {}\n",
    "pos_ex_dC_by_dweights = {}\n",
    "neg_ex_dC_by_dweights = {}\n",
    "pos_ex_dC_by_dbiases = {}\n",
    "neg_ex_dC_by_dbiases = {}\n",
    "mean_states = {}  # Running average of hidden layers\n",
    "softmax_weights = {}\n",
    "softmax_weights_grad = {}\n",
    "\n",
    "# Initialization Loop:\n",
    "for layer_num in range(args.num_layers):\n",
    "    d_in = args.layer_dims[layer_num]\n",
    "    d_out = args.layer_dims[layer_num + 1]\n",
    "    net_inputs[layer_num] = np.zeros([args.batch_size, d_out], dtype=args.dtype)\n",
    "    # Keeping track of all the input stuff at each layer\n",
    "    hidden_states[layer_num] = np.zeros([args.batch_size, d_out], dtype=args.dtype)\n",
    "    normed_states[layer_num] = np.zeros([args.batch_size, d_out], dtype=args.dtype)\n",
    "    pos_probs[layer_num] = np.zeros(args.batch_size, dtype=args.dtype)\n",
    "    neg_probs[layer_num] = np.zeros(args.batch_size, dtype=args.dtype)\n",
    "    # Keep track of the network information:\n",
    "    weights[layer_num] = (1/np.sqrt(d_in,  dtype=args.dtype) * np.random.randn(d_in, d_out))\n",
    "    weights_grad[layer_num] = np.zeros([d_in, d_out], dtype=args.dtype)\n",
    "    biases[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    biases_grad[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    pos_ex_dC_by_dweights[layer_num] = np.zeros([d_in, d_out], dtype=args.dtype)\n",
    "    neg_ex_dC_by_dweights[layer_num] = np.zeros([d_in, d_out], dtype=args.dtype)\n",
    "    pos_ex_dC_by_dbiases[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    neg_ex_dC_by_dbiases[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    mean_states[layer_num] = 0.5*np.ones([1, d_out], dtype=args.dtype)\n",
    "    softmax_weights[layer_num] = np.zeros([d_out, args.num_classes], dtype=args.dtype)\n",
    "    softmax_weights_grad[layer_num] = np.zeros([d_out, args.num_classes], dtype=args.dtype)\n",
    "\n",
    "batch_data.astype(args.dtype, copy=False)\n",
    "batch_targets.astype(args.dtype, copy=False)\n",
    "print(\"Done setting everything up\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9x/93y24nln7mjb1qs1mnw_0_f00000gn/T/ipykernel_32448/955794127.py:36: RuntimeWarning: overflow encountered in exp\n",
      "  return np.divide(1, 1 + np.exp(-1*x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Errors Epoch 0 = 10553\n",
      "Accuracy = 78.894%\n",
      "Train Errors Epoch 1 = 5796\n",
      "Accuracy = 88.408%\n",
      "Train Errors Epoch 2 = 4446\n",
      "Accuracy = 91.108%\n",
      "Train Errors Epoch 3 = 3789\n",
      "Accuracy = 92.422%\n",
      "Train Errors Epoch 4 = 3331\n",
      "Accuracy = 93.338%\n",
      "Train Errors Epoch 5 = 2924\n",
      "Accuracy = 94.152%\n",
      "Train Errors Epoch 6 = 2747\n",
      "Accuracy = 94.506%\n",
      "Train Errors Epoch 7 = 2868\n",
      "Accuracy = 94.264%\n",
      "Train Errors Epoch 8 = 2418\n",
      "Accuracy = 95.164%\n",
      "Train Errors Epoch 9 = 2233\n",
      "Accuracy = 95.534%\n",
      "Train Errors Epoch 10 = 2292\n",
      "Accuracy = 95.416%\n",
      "Train Errors Epoch 11 = 1993\n",
      "Accuracy = 96.014%\n",
      "Train Errors Epoch 12 = 1997\n",
      "Accuracy = 96.006%\n",
      "Train Errors Epoch 13 = 1688\n",
      "Accuracy = 96.624%\n",
      "Train Errors Epoch 14 = 1680\n",
      "Accuracy = 96.64%\n",
      "Train Errors Epoch 15 = 1690\n",
      "Accuracy = 96.62%\n",
      "Train Errors Epoch 16 = 1418\n",
      "Accuracy = 97.164%\n",
      "Train Errors Epoch 17 = 1333\n",
      "Accuracy = 97.334%\n",
      "Train Errors Epoch 18 = 1274\n",
      "Accuracy = 97.452%\n",
      "Train Errors Epoch 19 = 1152\n",
      "Accuracy = 97.696%\n",
      "Train Errors Epoch 20 = 1139\n",
      "Accuracy = 97.722%\n",
      "Train Errors Epoch 21 = 1155\n",
      "Accuracy = 97.69%\n",
      "Train Errors Epoch 22 = 1052\n",
      "Accuracy = 97.896%\n",
      "Train Errors Epoch 23 = 957\n",
      "Accuracy = 98.086%\n",
      "Train Errors Epoch 24 = 963\n",
      "Accuracy = 98.074%\n",
      "Train Errors Epoch 25 = 1120\n",
      "Accuracy = 97.76%\n",
      "Train Errors Epoch 26 = 976\n",
      "Accuracy = 98.048%\n",
      "Train Errors Epoch 27 = 876\n",
      "Accuracy = 98.248%\n",
      "Train Errors Epoch 28 = 777\n",
      "Accuracy = 98.446%\n",
      "Train Errors Epoch 29 = 751\n",
      "Accuracy = 98.498%\n",
      "Train Errors Epoch 30 = 692\n",
      "Accuracy = 98.616%\n",
      "Train Errors Epoch 31 = 693\n",
      "Accuracy = 98.614%\n",
      "Train Errors Epoch 32 = 671\n",
      "Accuracy = 98.658%\n",
      "Train Errors Epoch 33 = 665\n",
      "Accuracy = 98.67%\n",
      "Train Errors Epoch 34 = 628\n",
      "Accuracy = 98.744%\n",
      "Train Errors Epoch 35 = 702\n",
      "Accuracy = 98.596%\n",
      "Train Errors Epoch 36 = 615\n",
      "Accuracy = 98.77%\n",
      "Train Errors Epoch 37 = 568\n",
      "Accuracy = 98.864%\n",
      "Train Errors Epoch 38 = 658\n",
      "Accuracy = 98.684%\n",
      "Train Errors Epoch 39 = 504\n",
      "Accuracy = 98.992%\n",
      "Train Errors Epoch 40 = 574\n",
      "Accuracy = 98.852%\n",
      "Train Errors Epoch 41 = 620\n",
      "Accuracy = 98.76%\n",
      "Train Errors Epoch 42 = 478\n",
      "Accuracy = 99.044%\n",
      "Train Errors Epoch 43 = 507\n",
      "Accuracy = 98.986%\n",
      "Train Errors Epoch 44 = 473\n",
      "Accuracy = 99.054%\n",
      "Train Errors Epoch 45 = 522\n",
      "Accuracy = 98.956%\n",
      "Train Errors Epoch 46 = 503\n",
      "Accuracy = 98.994%\n",
      "Train Errors Epoch 47 = 430\n",
      "Accuracy = 99.14%\n",
      "Train Errors Epoch 48 = 396\n",
      "Accuracy = 99.208%\n",
      "Train Errors Epoch 49 = 390\n",
      "Accuracy = 99.22%\n",
      "Train Errors Epoch 50 = 366\n",
      "Accuracy = 99.268%\n",
      "Train Errors Epoch 51 = 385\n",
      "Accuracy = 99.23%\n",
      "Train Errors Epoch 52 = 360\n",
      "Accuracy = 99.28%\n",
      "Train Errors Epoch 53 = 294\n",
      "Accuracy = 99.412%\n",
      "Train Errors Epoch 54 = 286\n",
      "Accuracy = 99.428%\n",
      "Train Errors Epoch 55 = 274\n",
      "Accuracy = 99.452%\n",
      "Train Errors Epoch 56 = 251\n",
      "Accuracy = 99.498%\n",
      "Train Errors Epoch 57 = 216\n",
      "Accuracy = 99.568%\n",
      "Train Errors Epoch 58 = 218\n",
      "Accuracy = 99.564%\n",
      "Train Errors Epoch 59 = 193\n",
      "Accuracy = 99.614%\n",
      "Train Errors Epoch 60 = 167\n",
      "Accuracy = 99.666%\n",
      "Train Errors Epoch 61 = 158\n",
      "Accuracy = 99.684%\n",
      "Train Errors Epoch 62 = 139\n",
      "Accuracy = 99.722%\n",
      "Train Errors Epoch 63 = 132\n",
      "Accuracy = 99.736%\n",
      "Train Errors Epoch 64 = 141\n",
      "Accuracy = 99.718%\n",
      "Train Errors Epoch 65 = 109\n",
      "Accuracy = 99.782%\n",
      "Train Errors Epoch 66 = 104\n",
      "Accuracy = 99.792%\n",
      "Train Errors Epoch 67 = 79\n",
      "Accuracy = 99.842%\n",
      "Train Errors Epoch 68 = 82\n",
      "Accuracy = 99.836%\n",
      "Train Errors Epoch 69 = 81\n",
      "Accuracy = 99.838%\n",
      "Train Errors Epoch 70 = 67\n",
      "Accuracy = 99.866%\n",
      "Train Errors Epoch 71 = 68\n",
      "Accuracy = 99.864%\n",
      "Train Errors Epoch 72 = 51\n",
      "Accuracy = 99.898%\n",
      "Train Errors Epoch 73 = 52\n",
      "Accuracy = 99.896%\n",
      "Train Errors Epoch 74 = 42\n",
      "Accuracy = 99.916%\n",
      "Train Errors Epoch 75 = 41\n",
      "Accuracy = 99.918%\n",
      "Train Errors Epoch 76 = 31\n",
      "Accuracy = 99.938%\n",
      "Train Errors Epoch 77 = 25\n",
      "Accuracy = 99.95%\n",
      "Train Errors Epoch 78 = 31\n",
      "Accuracy = 99.938%\n",
      "Train Errors Epoch 79 = 30\n",
      "Accuracy = 99.94%\n",
      "Train Errors Epoch 80 = 17\n",
      "Accuracy = 99.966%\n",
      "Train Errors Epoch 81 = 10\n",
      "Accuracy = 99.98%\n",
      "Train Errors Epoch 82 = 12\n",
      "Accuracy = 99.976%\n",
      "Train Errors Epoch 83 = 17\n",
      "Accuracy = 99.966%\n",
      "Train Errors Epoch 84 = 12\n",
      "Accuracy = 99.976%\n",
      "Train Errors Epoch 85 = 14\n",
      "Accuracy = 99.972%\n",
      "Train Errors Epoch 86 = 12\n",
      "Accuracy = 99.976%\n",
      "Train Errors Epoch 87 = 6\n",
      "Accuracy = 99.988%\n",
      "Train Errors Epoch 88 = 5\n",
      "Accuracy = 99.99%\n",
      "Train Errors Epoch 89 = 9\n",
      "Accuracy = 99.982%\n",
      "Train Errors Epoch 90 = 7\n",
      "Accuracy = 99.986%\n",
      "Train Errors Epoch 91 = 5\n",
      "Accuracy = 99.99%\n",
      "Train Errors Epoch 92 = 6\n",
      "Accuracy = 99.988%\n",
      "Train Errors Epoch 93 = 5\n",
      "Accuracy = 99.99%\n",
      "Train Errors Epoch 94 = 3\n",
      "Accuracy = 99.994%\n",
      "Train Errors Epoch 95 = 2\n",
      "Accuracy = 99.996%\n",
      "Train Errors Epoch 96 = 4\n",
      "Accuracy = 99.992%\n",
      "Train Errors Epoch 97 = 2\n",
      "Accuracy = 99.996%\n",
      "Train Errors Epoch 98 = 3\n",
      "Accuracy = 99.994%\n",
      "Train Errors Epoch 99 = 3\n",
      "Accuracy = 99.994%\n",
      "Donesies Train Errors = 3\n",
      "Accuracy = 99.994%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.num_epochs):\n",
    "    pos_errors = np.zeros(args.num_layers, dtype=args.dtype)\n",
    "    neg_errors = np.zeros(args.num_layers, dtype=args.dtype)\n",
    "    pair_sum_errors = np.zeros(args.num_layers, dtype=args.dtype)\n",
    "    train_log_cost = 0\n",
    "    train_errors = 0\n",
    "    #  Linear decaying lr after first half of training; Hinton's \"epsgain\" parameter\n",
    "    if epoch < args.num_epochs/2.0:\n",
    "        weight_mult = 1 # This is Hinton's \"epsgain\" parameter\n",
    "    else:\n",
    "        weight_mult = (1 + 2.0*(args.num_epochs - epoch))/args.num_epochs\n",
    "    for batch in range(args.num_batches):\n",
    "        x = batch_data[:, :, batch]\n",
    "        y = batch_targets[:, :, batch]\n",
    "        # Now add the target label to the image (in the first few pixels)\n",
    "        x[:, 0:args.num_classes] = args.label_strength * y\n",
    "        normed_states[-1] = norm_rows(x)\n",
    "        for layer_num in range(args.num_layers):\n",
    "            # Forward Pass for each layer\n",
    "            net_inputs[layer_num] = normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num]\n",
    "            # ReLU\n",
    "            relu_output = np.maximum(net_inputs[layer_num], 0)\n",
    "            hidden_states[layer_num] = relu_output\n",
    "            normed_states[layer_num] = norm_rows(hidden_states[layer_num])\n",
    "            goodness = np.sum(hidden_states[layer_num]**2, axis=1, keepdims=True)\n",
    "            # Really not sure why we are subtracting this\n",
    "            pos_probs[layer_num] = (logistic_fn(goodness - (args.layer_dims[layer_num + 1]/args.temp)))\n",
    "            dC_by_din[layer_num] = np.tile(1 - pos_probs[layer_num], (1, args.layer_dims[layer_num + 1]))*relu_output\n",
    "            mean_states[layer_num] = 0.9*mean_states[layer_num] + 0.1*np.mean(relu_output, axis=0, keepdims=True)\n",
    "            # Regularizer encouraging layers to turn on\n",
    "            dC_by_din[layer_num] += args.lambda_mean*(np.mean(mean_states[layer_num]) - mean_states[layer_num])\n",
    "            pos_ex_dC_by_dweights[layer_num] = normed_states[layer_num - 1].T @ dC_by_din[layer_num]\n",
    "            pos_ex_dC_by_dbiases[layer_num] = np.sum(dC_by_din[layer_num], axis=0, keepdims=True)\n",
    "        # Now get hidden states when label is neutral.  Use this to pick hard negative labels\n",
    "        x[:, 0:args.num_classes] = args.label_strength * np.ones([args.batch_size, args.num_classes], dtype=args.dtype)/args.num_classes\n",
    "        normed_states[-1] = norm_rows(x)\n",
    "        # Run the neutral data through the forward pass\n",
    "        for layer_num in range(args.num_layers):\n",
    "            net_inputs[layer_num] = normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num]\n",
    "            relu_output = np.maximum(net_inputs[layer_num], 0)\n",
    "            hidden_states[layer_num] = relu_output\n",
    "            normed_states[layer_num] = norm_rows(relu_output)\n",
    "        # Looks like we use the last layer to calculate the negative data, some sort of feedback\n",
    "        label_in = np.tile(biases[args.num_layers - 1], (args.batch_size, 1))\n",
    "        for layer_num in range(args.min_layer_softmax, args.num_layers):\n",
    "            label_in += normed_states[layer_num] @ softmax_weights[layer_num]\n",
    "        label_in = label_in - np.tile(np.max(label_in, axis=1, keepdims=True), (1, args.num_classes))\n",
    "        unnorm_probs = np.exp(label_in)\n",
    "        train_predictions = unnorm_probs/np.tile(np.sum(unnorm_probs, axis=1, keepdims=True), (1, args.num_classes))\n",
    "        correct_probs = np.sum(train_predictions*y, axis=1, keepdims=True) # Should be a Column vector\n",
    "        curr_train_log_cost = -1 * np.log(args.tiny + correct_probs)\n",
    "        train_log_cost += np.sum(curr_train_log_cost)/args.num_batches\n",
    "        train_guesses = np.argmax(train_predictions, axis=1)\n",
    "        target_indices = np.argmax(y, axis=1)\n",
    "        train_errors += np.sum(train_guesses != target_indices)\n",
    "        # Now do the backprop step\n",
    "        dC_by_din[args.num_layers] = y - train_predictions\n",
    "        # Not used:\n",
    "        # dC_by_dbiases = sum(dC_by_din[args.num_layers], axis=0)\n",
    "        for layer_num in range(args.min_layer_softmax, args.num_layers):\n",
    "            dC_by_softmax_weights = normed_states[layer_num].T @ dC_by_din[args.num_layers]\n",
    "            softmax_weights_grad[layer_num] = args.grad_smoothing * softmax_weights_grad[layer_num] + (1 - args.grad_smoothing)*dC_by_softmax_weights/args.batch_size\n",
    "            softmax_weights[layer_num] += weight_mult*args.lr_softmax*(softmax_weights_grad[layer_num] - args.wc_softmax*softmax_weights[layer_num])\n",
    "        # Make Negative Data\n",
    "        neg_data = x\n",
    "        # Big negative logits\n",
    "        label_in_others = label_in - 1000*y\n",
    "        neg_data[:, :args.num_classes] = args.label_strength*choose_from_probs(np.exp(label_in_others)/sum(np.exp(label_in_others)))\n",
    "        normed_states[-1] = norm_rows(neg_data)\n",
    "        for layer_num in range(args.num_layers):\n",
    "            net_inputs[layer_num] = normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num]\n",
    "            relu_output = np.maximum(net_inputs[layer_num], 0)\n",
    "            hidden_states[layer_num] = relu_output\n",
    "            normed_states[layer_num] = norm_rows(relu_output)\n",
    "            goodness = np.sum(hidden_states[layer_num]**2, axis=1, keepdims=True)\n",
    "            # Really not sure why we are subtracting this\n",
    "            neg_probs[layer_num] = (logistic_fn(goodness - (args.layer_dims[layer_num + 1]/args.temp)))\n",
    "            dC_by_din[layer_num] = np.tile(-1*neg_probs[layer_num], (1, args.layer_dims[layer_num + 1]))*relu_output\n",
    "            neg_ex_dC_by_dweights[layer_num] = normed_states[layer_num - 1].T @ dC_by_din[layer_num]\n",
    "            neg_ex_dC_by_dbiases[layer_num] = np.sum(dC_by_din[layer_num], axis=0, keepdims=True)\n",
    "            pair_sum_errors[layer_num] += np.sum(neg_probs[layer_num] > pos_probs[layer_num])\n",
    "        # Gradient Backprop (FINALLY)\n",
    "        for layer_num in range(args.num_layers):\n",
    "            dC_by_dW = (pos_ex_dC_by_dweights[layer_num] + neg_ex_dC_by_dweights[layer_num])/args.batch_size\n",
    "            weights_grad[layer_num] = args.grad_smoothing*weights_grad[layer_num] + (1 - args.grad_smoothing)*(dC_by_dW)\n",
    "            \n",
    "            dC_by_dB = (pos_ex_dC_by_dbiases[layer_num] + neg_ex_dC_by_dbiases[layer_num])/args.batch_size\n",
    "            biases_grad[layer_num] = args.grad_smoothing*biases_grad[layer_num] + (1 - args.grad_smoothing)*dC_by_dB\n",
    "            biases[layer_num] += args.lr_hidden*weight_mult*biases_grad[layer_num]\n",
    "            weights[layer_num] += args.lr_hidden*weight_mult*(weights_grad[layer_num] - args.wc_hidden*weights[layer_num])\n",
    "\n",
    "    print(f\"Train Errors Epoch {epoch} = {train_errors}\")\n",
    "    print(f\"Accuracy = {100.0*(args.trainset_size-train_errors)/(1.0*args.trainset_size)}%\")\n",
    "print(f\"Donesies Train Errors = {train_errors}\")\n",
    "print(f\"Accuracy = {100.0*(args.trainset_size-train_errors)/(1.0*args.trainset_size)}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing errors = 175\n",
      "Test error percentage = 1.75\n",
      "Test Accuracy = 98.25\n"
     ]
    }
   ],
   "source": [
    "def calc_forward_errors(x_batch, y_batch, args):\n",
    "    normed_states = {}\n",
    "    hidden_states = {}\n",
    "    test_batch_size, _, num_batches = x_batch.shape\n",
    "    num_test_data = test_batch_size*num_batches\n",
    "    num_errors = 0\n",
    "    for batch_idx in range(num_batches):\n",
    "        x = x_batch[:, :, batch_idx]\n",
    "        y = y_batch[:, :, batch_idx]\n",
    "        act_sum_sqrs = np.zeros([num_batches, args.num_classes])\n",
    "        for label_num in range(args.num_classes):\n",
    "            x[:, :args.num_classes] = np.zeros([num_batches, args.num_classes])\n",
    "            x[:, label_num] = args.label_strength * np.ones([num_batches])\n",
    "            normed_states[-1] = norm_rows(x)\n",
    "            for layer_num in range(args.num_layers):\n",
    "                hidden_states[layer_num] = np.maximum(0, normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num])\n",
    "                if layer_num >= args.min_layer_energy:\n",
    "                    act_sum_sqrs[:, label_num] += np.sum(hidden_states[layer_num]**2, axis=1)\n",
    "                normed_states[layer_num] = norm_rows(hidden_states[layer_num])\n",
    "        guesses = np.argmax(act_sum_sqrs, axis=1)\n",
    "        targets_argmax = np.argmax(y, axis=1)\n",
    "        num_errors += np.sum(guesses != targets_argmax)\n",
    "    return num_errors, num_test_data\n",
    "\n",
    "def calc_softmax_errors(x_batch, y_batch, args):\n",
    "    normed_states = {}\n",
    "    test_batch_size, _, num_batches = x_batch.shape\n",
    "    num_test_data = test_batch_size*num_batches\n",
    "    num_errors = 0\n",
    "    log_cost = 0\n",
    "    for batch_idx in range(num_batches):\n",
    "        x = x_batch[:, :, batch_idx]\n",
    "        x[:, :args.num_classes] = args.label_strength * np.ones([test_batch_size, args.num_classes])/args.num_classes\n",
    "        y = y_batch[:, :, batch_idx]\n",
    "        normed_states[-1] = norm_rows(x)\n",
    "        for layer_num in range(args.num_layers):\n",
    "            hidden_state = np.maximum(0, normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num])\n",
    "            normed_states[layer_num] = norm_rows(hidden_state)\n",
    "        label_in = np.tile(biases[args.num_layers - 1], (test_batch_size, 1))\n",
    "        # Now do the forward pass using the labels from here\n",
    "        for layer_num in range(args.min_layer_softmax, args.num_layers):\n",
    "            label_in += normed_states[layer_num] @ softmax_weights[layer_num]\n",
    "        label_in -= np.tile(np.max(label_in, axis=1, keepdims=True), (1, args.num_classes))\n",
    "        # Can probably replace this with a softmax:\n",
    "        unnorm_probs = np.exp(label_in)\n",
    "        preds = unnorm_probs/np.tile(np.sum(unnorm_probs, axis=1, keepdims=True), (1, args.num_classes))\n",
    "        guesses = np.argmax(preds, axis=1)\n",
    "        targets_argmax = np.argmax(y, axis=1)\n",
    "        num_errors += np.sum(guesses != targets_argmax)\n",
    "        # Calculate the\n",
    "        log_costs_batch = -1*np.sum(np.sum(y * np.log(args.tiny + preds), axis=1, keepdims=True), axis=1, keepdims=True)\n",
    "        log_cost += np.sum(log_costs_batch)/num_batches\n",
    "    return num_errors, num_test_data, log_cost\n",
    "\n",
    "\n",
    "test_x = matdat['finaltestbatchdata'].astype(args.dtype, copy=False)\n",
    "test_y = matdat['finaltestbatchtargets'].astype(args.dtype, copy=False)\n",
    "# test_errors, num_test_data, = calc_forward_errors(x_batch=test_x, y_batch=test_y, args=args)\n",
    "test_errors, num_test_data, _ = calc_softmax_errors(x_batch=test_x, y_batch=test_y, args=args)\n",
    "\n",
    "\n",
    "print(f\"Number of testing errors = {test_errors}\")\n",
    "print(f\"Test error percentage = {test_errors*100.0/num_test_data}\")\n",
    "print(f\"Test Accuracy = {100 - (test_errors*100.0/num_test_data)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Viewing the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x[3, :].reshape(28, 28), cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04ffba0d41b4216980bf6c7d893a68b19ea460a3309261abb88c05c7a783f5c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
