{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "matdat = loadmat(\"mnistdata.mat\")\n",
    "batch_data = matdat[\"batchdata\"]\n",
    "batch_targets = matdat[\"batchtargets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, d_in, num_batches = batch_data.shape\n",
    "_, num_labels, _= batch_targets.shape\n",
    "\n",
    "class Args():\n",
    "    d_in = d_in\n",
    "    batch_size = batch_size\n",
    "    num_batches = num_batches\n",
    "    trainset_size = batch_size*num_batches\n",
    "    num_labels = num_labels\n",
    "    label_strength = 1 # Hinton's \"labelstrength\", the strength of the layer pixel\n",
    "    min_layer_softmax = 2 # Does not use hidden layers lower than this\n",
    "    min_level_energy = 2 # Used for computing goodness at test time\n",
    "    wc_hidden = 0.001   # Hinton's \"wc\" for forward weights\n",
    "    wc_softmax = 0.003 # Hinton's \"sup_wc\" for label prediction\n",
    "    lr_hidden = 0.01 # Hinton's \"epsilon\"\n",
    "    lr_softmax = 0.1 # Hinton's \"epsilonup\"\n",
    "    grad_smoothing = 0.9 # Hinton's \"delay\" variable, default=0.9\n",
    "    lambda_mean = 0.03 # Peer normalization (WTF is this?)\n",
    "    num_epochs = 100    # Maximum number of epochs; Hinton uses 100\n",
    "    layer_dims = [d_in, 1000, 1000, 1000, num_labels] # Layer sizes for the model (always start with d_in and end with num_lables)\n",
    "    num_layers = len(layer_dims)\n",
    "    temp = 1 # Used for rescaling weights (Not used)\n",
    "    tiny = np.exp(-50)\n",
    "    dtype=np.float32 # Was trying to speed this up, doesn't help much\n",
    "args = Args()\n",
    "\n",
    "def norm_rows(x):\n",
    "    # Makes the sum of squared activations be 1 per neuron\n",
    "    eps = np.exp(-100,  dtype=args.dtype)\n",
    "    num_comp = x.shape[1]\n",
    "    # Considering using vectors of ones instead\n",
    "    # Need to use keepdims for broadcasting\n",
    "    return x / np.tile(eps + np.mean(x ** 2, axis=1, keepdims=True)**(0.5), (1, num_comp))\n",
    "\n",
    "def logistic_fn(x):\n",
    "    return np.divide(1, 1 + np.exp(-1*x))\n",
    "\n",
    "\n",
    "def choose_from_probs(probs):\n",
    "    batch_size, num_labs = probs.shape\n",
    "    rands = np.random.rand(batch_size)\n",
    "    choices = np.zeros(probs.shape, dtype=args.dtype)\n",
    "    for n in range(batch_size):\n",
    "        sum_so_far = 0\n",
    "        used = 0\n",
    "        for lab_idx in range(num_labs):\n",
    "            sum_so_far += probs[n, lab_idx]\n",
    "            if rands[n] < sum_so_far and used == 0:\n",
    "                used = 1\n",
    "                choices[n, lab_idx] = 1\n",
    "                break\n",
    "    return choices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done setting everything up\n"
     ]
    }
   ],
   "source": [
    "net_inputs = {}\n",
    "hidden_states = {}\n",
    "normed_states = {}\n",
    "pos_probs = {}\n",
    "neg_probs = {}\n",
    "weights = {}\n",
    "weights_grad = {}\n",
    "biases = {}\n",
    "biases_grad = {}\n",
    "dC_by_din = {}\n",
    "pos_ex_dC_by_dweights = {}\n",
    "neg_ex_dC_by_dweights = {}\n",
    "pos_ex_dC_by_dbiases = {}\n",
    "neg_ex_dC_by_dbiases = {}\n",
    "mean_states = {}  # Running average of hidden layers\n",
    "softmax_weights = {}\n",
    "softmax_weights_grad = {}\n",
    "\n",
    "# Initialization Loop:\n",
    "for layer_num in range(1, args.num_layers):\n",
    "    d_in = args.layer_dims[layer_num - 1]\n",
    "    d_out = args.layer_dims[layer_num]\n",
    "    net_inputs[layer_num] = np.zeros([args.batch_size, d_out], dtype=args.dtype)\n",
    "    # Keeping track of all the input stuff at each layer\n",
    "    hidden_states[layer_num] = np.zeros([args.batch_size, d_out], dtype=args.dtype)\n",
    "    normed_states[layer_num] = np.zeros([args.batch_size, d_out], dtype=args.dtype)\n",
    "    pos_probs[layer_num] = np.zeros(args.batch_size, dtype=args.dtype)\n",
    "    neg_probs[layer_num] = np.zeros(args.batch_size, dtype=args.dtype)\n",
    "    # Keep track of the network information:\n",
    "    weights[layer_num] = (1/np.sqrt(d_in,  dtype=args.dtype) * np.random.randn(d_in, d_out))\n",
    "    weights_grad[layer_num] = np.zeros([d_in, d_out], dtype=args.dtype)\n",
    "    biases[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    biases_grad[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    pos_ex_dC_by_dweights[layer_num] = np.zeros([d_in, d_out], dtype=args.dtype)\n",
    "    neg_ex_dC_by_dweights[layer_num] = np.zeros([d_in, d_out], dtype=args.dtype)\n",
    "    pos_ex_dC_by_dbiases[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    neg_ex_dC_by_dbiases[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "\n",
    "for layer_num in range(1, args.num_layers):\n",
    "    mean_states[layer_num] = 0.5*np.ones([1, args.layer_dims[layer_num]], dtype=args.dtype)\n",
    "    softmax_weights[layer_num] = np.zeros([args.layer_dims[layer_num], args.num_labels], dtype=args.dtype)\n",
    "    softmax_weights_grad[layer_num] = np.zeros([args.layer_dims[layer_num], args.num_labels], dtype=args.dtype)\n",
    "\n",
    "batch_data.astype(args.dtype, copy=False)\n",
    "batch_targets.astype(args.dtype, copy=False)\n",
    "print(\"Done setting everything up\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/musicsketch/lib/python3.7/site-packages/ipykernel_launcher.py:36: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Errors Epoch 0 = 10205\n",
      "Accuracy = 79.59%\n",
      "Train Errors Epoch 1 = 5529\n",
      "Accuracy = 88.942%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9x/93y24nln7mjb1qs1mnw_0_f00000gn/T/ipykernel_1916/2415377504.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mbiases_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_smoothing\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbiases_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdC_by_dB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_hidden\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mweight_mult\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbiases_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_hidden\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mweight_mult\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwc_hidden\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train Errors Epoch {epoch} = {train_errors}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Accuracy = {100.0*(args.trainset_size-train_errors)/(1.0*args.trainset_size)}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(args.num_epochs):\n",
    "    pos_errors = np.zeros(args.num_layers, dtype=args.dtype)\n",
    "    neg_errors = np.zeros(args.num_layers, dtype=args.dtype)\n",
    "    pair_sum_errors = np.zeros(args.num_layers, dtype=args.dtype)\n",
    "    train_log_cost = 0\n",
    "    train_errors = 0\n",
    "    #  Linear decaying lr after first half of training; Hinton's \"epsgain\" parameter\n",
    "    if epoch < args.num_epochs/2.0:\n",
    "        weight_mult = 1 # This is Hinton's \"epsgain\" parameter\n",
    "    else:\n",
    "        weight_mult = (1 + 2.0*(args.num_epochs - epoch))/args.num_epochs\n",
    "    for batch in range(args.num_batches):\n",
    "        x = batch_data[:, :, batch]\n",
    "        y = batch_targets[:, :, batch]\n",
    "        # Now add the target label to the image (in the first few pixels)\n",
    "        x[:, 0:args.num_labels] = args.label_strength * y\n",
    "        normed_states[0] = norm_rows(x)\n",
    "        for layer_num in range(1, args.num_layers - 1):\n",
    "            # Forward Pass for each layer\n",
    "            net_inputs[layer_num] = normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num]\n",
    "            # ReLU\n",
    "            relu_output = np.maximum(net_inputs[layer_num], 0)\n",
    "            hidden_states[layer_num] = relu_output\n",
    "            normed_states[layer_num] = norm_rows(hidden_states[layer_num])\n",
    "            goodness = np.sum(hidden_states[layer_num]**2, axis=1, keepdims=True)\n",
    "            # Really not sure why we are subtracting this\n",
    "            pos_probs[layer_num] = (logistic_fn(goodness - (args.layer_dims[layer_num]/args.temp)))\n",
    "            dC_by_din[layer_num] = np.tile(1 - pos_probs[layer_num], (1, args.layer_dims[layer_num]))*relu_output\n",
    "            mean_states[layer_num] = 0.9*mean_states[layer_num] + 0.1*np.mean(relu_output, axis=0, keepdims=True)\n",
    "            # Regularizer encouraging layers to turn on\n",
    "            dC_by_din[layer_num] += args.lambda_mean*(np.mean(mean_states[layer_num]) - mean_states[layer_num])\n",
    "            pos_ex_dC_by_dweights[layer_num] = normed_states[layer_num - 1].T @ dC_by_din[layer_num]\n",
    "            pos_ex_dC_by_dbiases[layer_num] = np.sum(dC_by_din[layer_num], axis=0, keepdims=True)\n",
    "        # Now get hidden states when label is neutral.  Use this to pick hard negative labels\n",
    "        x[:, 0:args.num_labels] = args.label_strength * np.ones([args.batch_size, args.num_labels], dtype=args.dtype)/args.num_labels\n",
    "        normed_states[0] = norm_rows(x)\n",
    "        # Run the neutral data through the forward pass\n",
    "        for layer_num in range(1, args.num_layers - 1):\n",
    "            net_inputs[layer_num] = normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num]\n",
    "            relu_output = np.maximum(net_inputs[layer_num], 0)\n",
    "            hidden_states[layer_num] = relu_output\n",
    "            normed_states[layer_num] = norm_rows(relu_output)\n",
    "        # Looks like we use the last layer to calculate the negative data, some sort of feedback\n",
    "        label_in = np.tile(biases[args.num_layers - 1], (args.batch_size, 1))\n",
    "        for layer_num in range(args.min_layer_softmax, args.num_layers - 1):\n",
    "            label_in += normed_states[layer_num] @ softmax_weights[layer_num]\n",
    "        label_in = label_in - np.tile(np.max(label_in, axis=1, keepdims=True), (1, args.num_labels))\n",
    "        unnorm_probs = np.exp(label_in)\n",
    "        train_predictions = unnorm_probs/np.tile(np.sum(unnorm_probs, axis=1, keepdims=True), (1, args.num_labels))\n",
    "        correct_probs = np.sum(train_predictions*y, axis=1, keepdims=True) # Should be a Column vector\n",
    "        curr_train_log_cost = -1 * np.log(args.tiny + correct_probs)\n",
    "        train_log_cost += np.sum(curr_train_log_cost)/args.num_batches\n",
    "        train_guesses = np.argmax(train_predictions, axis=1)\n",
    "        target_indices = np.argmax(y, axis=1)\n",
    "        train_errors += np.sum(train_guesses != target_indices)\n",
    "        # Now do the backprop step\n",
    "        dC_by_din[args.num_layers] = y - train_predictions\n",
    "        # Not used:\n",
    "        # dC_by_dbiases = sum(dC_by_din[args.num_layers], axis=0)\n",
    "        for layer_num in range(args.min_layer_softmax, args.num_layers):\n",
    "            dC_by_softmax_weights = normed_states[layer_num].T @ dC_by_din[args.num_layers]\n",
    "            softmax_weights_grad[layer_num] = args.grad_smoothing * softmax_weights_grad[layer_num] + (1 - args.grad_smoothing)*dC_by_softmax_weights/args.batch_size\n",
    "            softmax_weights[layer_num] += weight_mult*args.lr_softmax*(softmax_weights_grad[layer_num] - args.wc_softmax*softmax_weights[layer_num])\n",
    "        # Make Negative Data\n",
    "        neg_data = x\n",
    "        # Big negative logits\n",
    "        label_in_others = label_in - 1000*y\n",
    "        neg_data[:, :args.num_labels] = args.label_strength*choose_from_probs(np.exp(label_in_others)/sum(np.exp(label_in_others)))\n",
    "        normed_states[0] = norm_rows(neg_data)\n",
    "        for layer_num in range(1, args.num_layers - 1):\n",
    "            net_inputs[layer_num] = normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num]\n",
    "            relu_output = np.maximum(net_inputs[layer_num], 0)\n",
    "            hidden_states[layer_num] = relu_output\n",
    "            normed_states[layer_num] = norm_rows(relu_output)\n",
    "            goodness = np.sum(hidden_states[layer_num]**2, axis=1, keepdims=True)\n",
    "            # Really not sure why we are subtracting this\n",
    "            neg_probs[layer_num] = (logistic_fn(goodness - (args.layer_dims[layer_num]/args.temp)))\n",
    "            dC_by_din[layer_num] = np.tile(-1*neg_probs[layer_num], (1, args.layer_dims[layer_num]))*relu_output\n",
    "            neg_ex_dC_by_dweights[layer_num] = normed_states[layer_num - 1].T @ dC_by_din[layer_num]\n",
    "            neg_ex_dC_by_dbiases[layer_num] = np.sum(dC_by_din[layer_num], axis=0, keepdims=True)\n",
    "            pair_sum_errors[layer_num] += np.sum(neg_probs[layer_num] > pos_probs[layer_num])\n",
    "        # Gradient Backprop (FINALLY)\n",
    "        for layer_num in range(1, args.num_layers - 1):\n",
    "            dC_by_dW = (pos_ex_dC_by_dweights[layer_num] + neg_ex_dC_by_dweights[layer_num])/args.batch_size\n",
    "            weights_grad[layer_num] = args.grad_smoothing*weights_grad[layer_num] + (1 - args.grad_smoothing)*(dC_by_dW)\n",
    "            \n",
    "            dC_by_dB = (pos_ex_dC_by_dbiases[layer_num] + neg_ex_dC_by_dbiases[layer_num])/args.batch_size\n",
    "            biases_grad[layer_num] = args.grad_smoothing*biases_grad[layer_num] + (1 - args.grad_smoothing)*dC_by_dB\n",
    "            biases[layer_num] += args.lr_hidden*weight_mult*biases_grad[layer_num]\n",
    "            weights[layer_num] += args.lr_hidden*weight_mult*(weights_grad[layer_num] - args.wc_hidden*weights[layer_num])\n",
    "    print(f\"Train Errors Epoch {epoch} = {train_errors}\")\n",
    "    print(f\"Accuracy = {100.0*(args.trainset_size-train_errors)/(1.0*args.trainset_size)}%\")\n",
    "print(f\"Donesies Train Errors = {train_errors}\")\n",
    "print(f\"Accuracy = {100.0*(args.trainset_size-train_errors)/(1.0*args.trainset_size)}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For viewing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8c222ce910>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x[3, :].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_in' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9x/93y24nln7mjb1qs1mnw_0_f00000gn/T/ipykernel_48816/2095956048.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabel_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'label_in' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(normed_states[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63a52d6771f90d6efee1706f1aff88626781cbd19f1875df20d35556461fbc38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
