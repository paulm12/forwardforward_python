{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "matdat = loadmat(\"mnistdata.mat\")\n",
    "batch_data = matdat[\"batchdata\"]\n",
    "batch_targets = matdat[\"batchtargets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, d_in, num_batches = batch_data.shape\n",
    "_, num_labels, _= batch_targets.shape\n",
    "\n",
    "class Args():\n",
    "    d_in = d_in\n",
    "    batch_size = batch_size\n",
    "    num_batches = num_batches\n",
    "    trainset_size = batch_size*num_batches\n",
    "    num_labels = num_labels\n",
    "    label_strength = 1 # Hinton's \"labelstrength\", the strength of the layer pixel\n",
    "    min_layer_softmax = 2 # Does not use hidden layers lower than this\n",
    "    min_level_energy = 2 # Used for computing goodness at test time\n",
    "    wc_hidden = 0.001   # Hinton's \"wc\" for forward weights\n",
    "    wc_softmax = 0.003 # Hinton's \"sup_wc\" for label prediction\n",
    "    lr_hidden = 0.01 # Hinton's \"epsilon\"\n",
    "    lr_softmax = 0.1 # Hinton's \"epsilonup\"\n",
    "    grad_smoothing = 0.9 # Hinton's \"delay\" variable, default=0.9\n",
    "    lambda_mean = 0.03 # Peer normalization (WTF is this?)\n",
    "    num_epochs = 100    # Maximum number of epochs; Hinton uses 100\n",
    "    layer_dims = [d_in, 1000, 1000, 1000, num_labels] # Layer sizes for the model (always start with d_in and end with num_lables)\n",
    "    num_layers = len(layer_dims)\n",
    "    temp = 1 # Used for rescaling weights (Not used)\n",
    "    tiny = np.exp(-50)\n",
    "    dtype=np.float32 # Was trying to speed this up, doesn't help much\n",
    "args = Args()\n",
    "\n",
    "def norm_rows(x):\n",
    "    # Makes the sum of squared activations be 1 per neuron\n",
    "    eps = np.exp(-100,  dtype=args.dtype)\n",
    "    num_comp = x.shape[1]\n",
    "    # Considering using vectors of ones instead\n",
    "    # Need to use keepdims for broadcasting\n",
    "    return x / np.tile(eps + np.mean(x ** 2, axis=1, keepdims=True)**(0.5), (1, num_comp))\n",
    "\n",
    "def logistic_fn(x):\n",
    "    return np.divide(1, 1 + np.exp(-1*x))\n",
    "\n",
    "\n",
    "def choose_from_probs(probs):\n",
    "    batch_size, num_labs = probs.shape\n",
    "    rands = np.random.rand(batch_size)\n",
    "    choices = np.zeros(probs.shape, dtype=args.dtype)\n",
    "    for n in range(batch_size):\n",
    "        sum_so_far = 0\n",
    "        used = 0\n",
    "        for lab_idx in range(num_labs):\n",
    "            sum_so_far += probs[n, lab_idx]\n",
    "            if rands[n] < sum_so_far and used == 0:\n",
    "                used = 1\n",
    "                choices[n, lab_idx] = 1\n",
    "                break\n",
    "    return choices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/musicsketch/lib/python3.7/site-packages/ipykernel_launcher.py:39: RuntimeWarning: overflow encountered in exp\n",
      "/opt/anaconda3/envs/musicsketch/lib/python3.7/site-packages/ipykernel_launcher.py:116: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Errors Epoch 0 = 40727\n",
      "Accuracy = 18.546%\n",
      "Train Errors Epoch 1 = 42517\n",
      "Accuracy = 14.966%\n",
      "Train Errors Epoch 2 = 43267\n",
      "Accuracy = 13.466%\n",
      "Train Errors Epoch 3 = 43482\n",
      "Accuracy = 13.036%\n",
      "Train Errors Epoch 4 = 44165\n",
      "Accuracy = 11.67%\n",
      "Train Errors Epoch 5 = 44491\n",
      "Accuracy = 11.018%\n",
      "Train Errors Epoch 6 = 44505\n",
      "Accuracy = 10.99%\n",
      "Train Errors Epoch 7 = 44574\n",
      "Accuracy = 10.852%\n",
      "Train Errors Epoch 8 = 44289\n",
      "Accuracy = 11.422%\n",
      "Train Errors Epoch 9 = 44273\n",
      "Accuracy = 11.454%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9x/93y24nln7mjb1qs1mnw_0_f00000gn/T/ipykernel_37894/489160730.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_layer_softmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mdC_by_softmax_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormed_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mdC_by_din\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0msoftmax_weights_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_smoothing\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msoftmax_weights_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdC_by_softmax_weights\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0msoftmax_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mweight_mult\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_softmax\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax_weights_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwc_softmax\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msoftmax_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# Make Negative Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net_inputs = {}\n",
    "hidden_states = {}\n",
    "normed_states = {}\n",
    "pos_probs = {}\n",
    "neg_probs = {}\n",
    "weights = {}\n",
    "weights_grad = {}\n",
    "biases = {}\n",
    "biases_grad = {}\n",
    "dC_by_din = {}\n",
    "pos_ex_dC_by_dweights = {}\n",
    "neg_ex_dC_by_dweights = {}\n",
    "pos_ex_dC_by_dbiases = {}\n",
    "neg_ex_dC_by_dbiases = {}\n",
    "mean_states = {}  # Running average of hidden layers\n",
    "softmax_weights = {}\n",
    "softmax_weights_grad = {}\n",
    "\n",
    "# Initialization Loop:\n",
    "for layer_num in range(1, args.num_layers):\n",
    "    d_in = args.layer_dims[layer_num - 1]\n",
    "    d_out = args.layer_dims[layer_num]\n",
    "    net_inputs[layer_num] = np.zeros([args.batch_size, d_out], dtype=args.dtype)\n",
    "    # Keeping track of all the input stuff at each layer\n",
    "    hidden_states[layer_num] = np.zeros([args.batch_size, d_out], dtype=args.dtype)\n",
    "    normed_states[layer_num] = np.zeros([args.batch_size, d_out], dtype=args.dtype)\n",
    "    pos_probs[layer_num] = np.zeros(args.batch_size, dtype=args.dtype)\n",
    "    neg_probs[layer_num] = np.zeros(args.batch_size, dtype=args.dtype)\n",
    "    # Keep track of the network information:\n",
    "    weights[layer_num] = (1/np.sqrt(d_in,  dtype=args.dtype) * np.random.randn(d_in, d_out))\n",
    "    weights_grad[layer_num] = np.zeros([d_in, d_out], dtype=args.dtype)\n",
    "    biases[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    biases_grad[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    pos_ex_dC_by_dweights[layer_num] = np.zeros([d_in, d_out], dtype=args.dtype)\n",
    "    neg_ex_dC_by_dweights[layer_num] = np.zeros([d_in, d_out], dtype=args.dtype)\n",
    "    pos_ex_dC_by_dbiases[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "    neg_ex_dC_by_dbiases[layer_num] = np.zeros([1, d_out], dtype=args.dtype)\n",
    "\n",
    "for layer_num in range(1, args.num_layers):\n",
    "    mean_states[layer_num] = 0.5*np.ones([1, args.layer_dims[layer_num]], dtype=args.dtype)\n",
    "    softmax_weights[layer_num] = np.zeros([args.layer_dims[layer_num], args.num_labels], dtype=args.dtype)\n",
    "    softmax_weights_grad[layer_num] = np.zeros([args.layer_dims[layer_num], args.num_labels], dtype=args.dtype)\n",
    "\n",
    "batch_data.astype(args.dtype, copy=False)\n",
    "batch_targets.astype(args.dtype, copy=False)\n",
    "print(\"Done setting everything up\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(args.num_epochs):\n",
    "    pos_errors = np.zeros(args.num_layers, dtype=args.dtype)\n",
    "    neg_errors = np.zeros(args.num_layers, dtype=args.dtype)\n",
    "    pair_sum_errors = np.zeros(args.num_layers, dtype=args.dtype)\n",
    "    train_log_cost = 0\n",
    "    train_errors = 0\n",
    "    #  Linear decaying lr after first half of training; Hinton's \"epsgain\" parameter\n",
    "    if epoch < args.num_epochs/2.0:\n",
    "        weight_mult = 1 # This is Hinton's \"epsgain\" parameter\n",
    "    else:\n",
    "        weight_mult = (1 + 2.0*(args.num_epochs - epoch))/args.num_epochs\n",
    "    for batch in range(args.num_batches):\n",
    "        x = batch_data[:, :, batch]\n",
    "        y = batch_targets[:, :, batch]\n",
    "        # Now add the target label to the image (in the first few pixels)\n",
    "        x[:, 0:args.num_labels] = args.label_strength * y\n",
    "        normed_states[0] = norm_rows(x)\n",
    "        for layer_num in range(1, args.num_layers - 1):\n",
    "            # Forward Pass for each layer\n",
    "            net_inputs[layer_num] = normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num]\n",
    "            # ReLU\n",
    "            relu_output = np.maximum(net_inputs[layer_num], 0)\n",
    "            hidden_states[layer_num] = relu_output\n",
    "            normed_states[layer_num] = norm_rows(hidden_states[layer_num])\n",
    "            goodness = np.sum(hidden_states[layer_num]**2, axis=1, keepdims=True)\n",
    "            # Really not sure why we are subtracting this\n",
    "            pos_probs[layer_num] = (logistic_fn(goodness - (args.layer_dims[layer_num]/args.temp)))\n",
    "            dC_by_din[layer_num] = np.tile(1 - pos_probs[layer_num], (1, args.layer_dims[layer_num]))*relu_output\n",
    "            mean_states[layer_num] = 0.9*mean_states[layer_num] + 0.1*np.mean(relu_output, axis=0, keepdims=True)\n",
    "            # Regularizer encouraging layers to turn on\n",
    "            dC_by_din[layer_num] += args.lambda_mean*(np.mean(mean_states[layer_num]) - mean_states[layer_num])\n",
    "            pos_ex_dC_by_dweights[layer_num] = normed_states[layer_num - 1].T @ dC_by_din[layer_num]\n",
    "            pos_ex_dC_by_dbiases[layer_num] = np.sum(dC_by_din[layer_num], axis=0, keepdims=True)\n",
    "        # Now get hidden states when label is neutral.  Use this to pick hard negative labels\n",
    "        x[:, 0:args.num_labels] = args.label_strength * np.ones([args.batch_size, args.num_labels], dtype=args.dtype)/args.num_labels\n",
    "        normed_states[0] = norm_rows(x)\n",
    "        # Run the neutral data through the forward pass\n",
    "        for layer_num in range(1, args.num_layers - 1):\n",
    "            net_inputs[layer_num] = normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num]\n",
    "            relu_output = np.maximum(net_inputs[layer_num], 0)\n",
    "            hidden_states[layer_num] = relu_output\n",
    "            normed_states[layer_num] = norm_rows(relu_output)\n",
    "        # Looks like we use the last layer to calculate the negative data, some sort of feedback\n",
    "        label_in = np.tile(biases[args.num_layers - 1], (args.batch_size, 1))\n",
    "        for layer_num in range(args.min_layer_softmax, args.num_layers - 1):\n",
    "            label_in += normed_states[layer_num] @ softmax_weights[layer_num]\n",
    "        label_in = label_in - np.tile(np.max(label_in, axis=1, keepdims=True), (1, args.num_labels))\n",
    "        unnorm_probs = np.exp(label_in)\n",
    "        train_predictions = unnorm_probs/np.tile(np.sum(unnorm_probs, axis=1, keepdims=True), (1, args.num_labels))\n",
    "        correct_probs = np.sum(train_predictions*y, axis=1, keepdims=True) # Should be a Column vector\n",
    "        curr_train_log_cost = -1 * np.log(args.tiny + correct_probs)\n",
    "        train_log_cost += np.sum(curr_train_log_cost)/args.num_batches\n",
    "        train_guesses = np.argmax(train_predictions, axis=1)\n",
    "        target_indices = np.argmax(y, axis=1)\n",
    "        train_errors += np.sum(train_guesses != target_indices)\n",
    "        # Now do the backprop step\n",
    "        dC_by_din[args.num_layers] = y - train_predictions\n",
    "        # Not used:\n",
    "        # dC_by_dbiases = sum(dC_by_din[args.num_layers], axis=0)\n",
    "        for layer_num in range(args.min_layer_softmax, args.num_layers):\n",
    "            dC_by_softmax_weights = normed_states[layer_num].T @ dC_by_din[args.num_layers]\n",
    "            softmax_weights_grad[layer_num] = args.grad_smoothing * softmax_weights_grad[layer_num] + (1 - args.grad_smoothing)*dC_by_softmax_weights/args.batch_size\n",
    "            softmax_weights[layer_num] += weight_mult*args.lr_softmax*(softmax_weights_grad[layer_num] - args.wc_softmax*softmax_weights[layer_num])\n",
    "        # Make Negative Data\n",
    "        neg_data = x\n",
    "        # Big negative logits\n",
    "        label_in_others = label_in - 1000*y\n",
    "        neg_data[:, :args.num_labels] = args.label_strength*choose_from_probs(np.exp(label_in_others)/sum(np.exp(label_in_others)))\n",
    "        normed_states[0] = norm_rows(neg_data)\n",
    "        for layer_num in range(1, args.num_layers - 1):\n",
    "            net_inputs[layer_num] = normed_states[layer_num - 1] @ weights[layer_num] + biases[layer_num]\n",
    "            relu_output = np.maximum(net_inputs[layer_num], 0)\n",
    "            hidden_states[layer_num] = relu_output\n",
    "            normed_states[layer_num] = norm_rows(relu_output)\n",
    "            goodness = np.sum(hidden_states[layer_num]**2, axis=1, keepdims=True)\n",
    "            # Really not sure why we are subtracting this\n",
    "            neg_probs[layer_num] = (logistic_fn(goodness - (args.layer_dims[layer_num]/args.temp)))\n",
    "            dC_by_din[layer_num] = np.tile(-1*neg_probs[layer_num], (1, args.layer_dims[layer_num]))*relu_output\n",
    "            neg_ex_dC_by_dweights[layer_num] = normed_states[layer_num - 1].T @ dC_by_din[layer_num]\n",
    "            neg_ex_dC_by_dbiases[layer_num] = np.sum(dC_by_din[layer_num], axis=0, keepdims=True)\n",
    "            pair_sum_errors[layer_num] += np.sum(neg_probs[layer_num] > pos_probs[layer_num])\n",
    "        # Gradient Backprop (FINALLY)\n",
    "        for layer_num in range(1, args.num_layers - 1):\n",
    "            dC_by_dW = (pos_ex_dC_by_dweights[layer_num] + neg_ex_dC_by_dweights[layer_num])/args.batch_size\n",
    "            weights_grad[layer_num] = args.grad_smoothing*weights_grad[layer_num] + (1 - args.grad_smoothing)*(dC_by_dW)\n",
    "            \n",
    "            dC_by_dB = (pos_ex_dC_by_dbiases[layer_num] + neg_ex_dC_by_dbiases[layer_num])/args.batch_size\n",
    "            biases_grad[layer_num] = args.grad_smoothing*biases_grad[layer_num] + (1 - args.grad_smoothing)*dC_by_dB\n",
    "            biases[layer_num] += args.lr_hidden*weight_mult*biases_grad[layer_num]\n",
    "            weights[layer_num] += args.lr_hidden*weight_mult*(weights_grad[layer_num] - args.wc_hidden*weights[layer_num])\n",
    "    print(f\"Train Errors Epoch {epoch} = {train_errors}\")\n",
    "    print(f\"Accuracy = {100.0*(args.trainset_size-train_errors)/(1.0*args.trainset_size)}%\")\n",
    "print(f\"Donesies Train Errors = {train_errors}\")\n",
    "print(f\"Accuracy = {100.0*(args.trainset_size-train_errors)/(1.0*args.trainset_size)}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For viewing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe5fd9395d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x[3, :].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63a52d6771f90d6efee1706f1aff88626781cbd19f1875df20d35556461fbc38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
